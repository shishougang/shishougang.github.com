<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Lock | (learn&think)]]></title>
  <link href="http://dreamrunner.org/tags/lock/atom.xml" rel="self"/>
  <link href="http://dreamrunner.org/"/>
  <updated>2014-09-01T22:20:56+08:00</updated>
  <id>http://dreamrunner.org/</id>
  <author>
    <name><![CDATA[DreamRunner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[浅谈Mutex (Lock)]]></title>
    <link href="http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock/"/>
    <updated>2014-06-29T20:52:09+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock</id>
    <content type="html"><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Mutual_exclusion">Mutex</a>(又叫Lock),在多线程中,作为同步的基本类型,用来保证没有两个线程或进程同时在他们的关键区域.因为Mutex这种排它性,很多人认为Mutex开销很大,尽量避免使用它.就如这篇
分析完共享数据问题后,进一步分析说明
<a href="http://courses.cs.washington.edu/courses/cse451/03wi/section/prodcons.htm">Avoiding locks</a>
来解决这个问题.但Mutex真的开销如此大,还是被大家误解了?Matthew
Dillon<a href="http://groups.google.com/group/net.micro.mac/msg/752d18de371bd65c?dmode=source">写道</a>,”Most
people have the misconception that locks are slow.”, Jeff Preshing也
<a href="http://preshing.com/20111118/locks-arent-slow-lock-contention-is/">写了这篇”Locks Aren’t Slow; Lock Contention Is”</a>.</p>

<p>那么接下来做3个关于Mutex的Benchmark,具体分析一下Mutex的开销如何,最后并
利用原子操作和semaphore实现一个lightweight Mutex.</p>

<!-- more -->

<p>一个Mutex仅仅从Lock到Unlock具体开销是多少,是不是占用很多时间,从
<a href="http://preshing.com/20111124/always-use-a-lightweight-mutex/">Always Use a Lightweight Mutex</a>
从可以看到在windows中有两种
Mutex:<a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms684266%28v=vs.85%29.aspx">Muetx</a>
和
<a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms682530%28v=vs.85%29.aspx">Critical Section</a>,
重量级和轻量级的区别,两者的时间开销相差25倍多,所以一直使用轻量级的Mutex.</p>

<p><a href="http://ridiculousfish.com/blog/posts/barrier.html">这篇文章</a>在高强度
下lock的性能:每个线程做任何事情都占用lock(高冲突),lock占用极短的时间
(高频率).值得一读,但是在实际应用中,基本避免如此使用locks.这里对
Mutex Contention和Mutex Frequency都做最好和最坏场景的使用测试.</p>

<p>Mutex被灌以避免使用也因为其他原因.现在有很多大家熟知的
<a href="en.wikipedia.org/wiki/Non-blocking_algorithma">lock-free programming</a>
技术.Lock-free编程非常具有挑战性,但在实际场景中获得巨大的性能.既然有
lock-free的技术吸引我们使用它们,那么locks就显得索然无味了.</p>

<p>但也不能因此忽略lock.因为在实际很多场景,它仍然是利器.</p>

<h2 id="lightweight-mutex-benchmark">Lightweight Mutex Benchmark</h2>

<p>Linux下的POSIX thread是轻量级的Mutex.基于Linux特有的
<a href="http://en.wikipedia.org/wiki/Futex">futex</a>技术,当没有其他线程竞争锁时它被优化过.使
用如下简单的例子,测试一个单线程lock和unlock,所有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/mutex_time">Github上</a>.</p>

<p><code>c++
pthread_mutex_init(&amp;lock, NULL);
const int kN = 1000000;
for (int i = 0; i &lt; kN; ++i) {
    pthread_mutex_lock(&amp;lock);
    pthread_mutex_unlock(&amp;lock);
}
pthread_mutex_destroy(&amp;lock);
</code></p>

<p>插入相应的时间代码,算出10万次的单线程lock/unlock平均时间.在不同的处理
器下,结果如下:</p>

<p><img class="center" src="/images/blog/2014/multithreading/mutex_benchmark.png" width="450" height="200" title="‘mutex_benchmark’" ></p>

<p>如果假设一个线程每分钟获取1e5次mutex,并且没有其他线程与它竞争.基于如下
的图,可预计0.2%到0.4%的开销.不算差.在比较低频率下,开销基本忽略不计.之
后<a href="http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock#build-own-lightweight-mutex">Build own lightweight mutex</a>,会利用<a href="http://en.wikipedia.org/wiki/Semaphore_%28programming%29">semaphore</a>和一个原子操作,实现一个lightweight mutex.</p>

<p>POSIX thread与Windows Critical Section不同,它不仅支持线程间的同步,
还支持进程间的同步.实例代码如下:</p>

<p>```  c++  mutex_between_process.cc
pthread_mutex_t mutex;
pthread_mutexattr_t attrmutex;</p>

<p>/* Initialise attribute to mutex. */
pthread_mutexattr_init(&amp;attrmutex);
pthread_mutexattr_setpshared(&amp;attrmutex, PTHREAD_PROCESS_SHARED);
pthread_mutex_init(&amp;mutex, &amp;attrmutex);</p>

<p>/* Use the mutex. */</p>

<p>/* Clean up. */
pthread_mutex_destroy(pmutex);
pthread_mutexattr_destroy(&amp;attrmutex);
```</p>

<h2 id="mutex-contention-benchmark">Mutex Contention Benchmark</h2>

<p>在测试中,产生一个不断生成随机数的线程,使用自己编制的线程安全的
<a href="http://en.wikipedia.org/wiki/Mersenne_twister">Mersenne Twister</a>实现
代码.每过一段时间,它获取和释放一个锁,获取和释放锁之间的时间每次是随机的,但
是总的平均时间是提前设计好的.这个随机的过程就是个泊松分布过程,计算出产
生一个随机数的平均时间6.25 ns在2.93 GHz i7上,把它作为运行单位.利用
<a href="http://wiki.dreamrunner.org/public_html/Algorithms/Theory%20of%20Algorithms/poisson-process.html">Poisson Process</a>
的算法决定运行多少个运行单位在获取和释放锁之间.并利用
<a href="http://dreamrunner.org/blog/2014/06/24/high-resolution-time/">High Resolution Time</a>API
计算时间.这个线程的代码如下,所有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/mutex_contention">Github上</a>:</p>

<p>``` c++
  GetMonotonicTime(&amp;start);
  for (;;) {
    work_units = static_cast<int> (random.PoissonInterval(
        global_state.average_unlock_count) + 0.5f);
    for (int i = 0; i &lt; work_units; ++i) {
      random.Integer();
    }
    thread_stats.workdone += work_units;</int></p>

<pre><code>GetMonotonicTime(&amp;end);
elapsed_time = GetElapsedTime(&amp;start, &amp;end);
if (elapsed_time &gt;= global_state.time_limit) {
  break;
}

// Do some work while holding the lock
pthread_mutex_lock(&amp;global_state.thread_mutex);
work_units = static_cast&lt;int&gt; (random.PoissonInterval(
    global_state.average_locked_count) + 0.5f);
for (int i = 0; i &lt; work_units; ++i) {
  random.Integer();
}
thread_stats.workdone += work_units;
pthread_mutex_unlock(&amp;global_state.thread_mutex);

thread_stats.iterations++;
GetMonotonicTime(&amp;end);
elapsed_time = GetElapsedTime(&amp;start, &amp;end);
if (elapsed_time &gt;= global_state.time_limit) {
  break;
}   } ```
</code></pre>

<p>这里模拟获取和释放15000次锁每秒,从1个线程运行到2个线程,最后到4个线
程.并且验证占用锁的时间,从0%到100%的每次运行时间占用锁.把1个线程的完成
的工作量作为基准数据,其他的去除以它,计算相对增益.基本测试方案如下:</p>

<p><code>c++
// Test 15000 locks per second: thread number, lock_interval
    1, 1/15000.0f, 
    2, 1/15000.0f,
    3, 1/15000.0f,
    4, 1/15000.0f,
</code></p>

<p><img src="/images/blog/2014/multithreading/lock_benchmark.png" title="lock_benchmark’" ></p>

<p>从图中看出,随着锁占用的时间增加,并行性越来越差,直到最后占用60%以后,单
线程运行的更好.可以说,短时间的占用锁的时间,以10%以内,系统达到很高的并
行性.虽然并不是完美的,但是也接近.锁总体很快.</p>

<p>把这个结果放到实际中,Jeff Preshing在
<a href="http://preshing.com/20111118/locks-arent-slow-lock-contention-is/">这篇</a>
提到,实际的游戏程序中,15000的锁每秒来自3个线程,占用锁的时间相对2%.在图
中很适中的区域.</p>

<h2 id="mutex-frequency-benchmark">Mutex Frequency Benchmark</h2>

<p>尽管一个lightweight mutex有开销,但如上测试在2.40GHz i5上,lock/unlock锁
开销约 <strong>34.2ns</strong> ,因此15000锁每秒开销很低以致不是严重影响结果.那么把
锁的每秒频率提高呢?</p>

<p>只创建2个线程,进行一系列的锁的每秒频率测试在2.40GHz i5上,从占用锁时间
10 ns(1e8/s)到100 us(1e4/s),用单线程的占用锁时间10 ms作为基准工作量,其
他与它比较,测试方案如下:</p>

<p>``` c++
  // Reference
  1, 10e-3f,      // 10 ms        100/s</p>

<pre><code>// Test various lock rates with 2 threads
2, 10e-9f,      // 10 ns        100000000/s
2, 31.6e-9f,    // 31.6 ns      31600000/s
2, 100e-9f,     // 100 ns       10000000/s
2, 316e-9f,     // 316 ns       3160000/s
2, 1e-6f,       // 1 us         1000000/s
2, 3.16e-6f,    // 3.16 us      316000/s
2, 10e-6f,      // 10 us        100000/s
2, 31.6e-6f,    // 31.6 us      31600/s
2, 100e-6f,     // 100 us       10000/s ```
</code></pre>

<p><img src="/images/blog/2014/multithreading/frequency_benchmark.png" title="‘frequency_bechmark’" ></p>

<p>如预想一样,对于非常高频率的锁,锁的开销开始减少实际工作量.在网络上,可以
找到很多同样的测试.图中下边的线条,对于这样高的频率,也就是占用锁的时间
很短,就一些CPU的指令,这样的情况下,当锁之间的工作如此简单,那么一个
lock-free的实现更适合.</p>

<p>我们获得了一大块关于锁的性能:从它进行很好的情况,到缓慢应用的情况.在考
虑实际锁的使用情况,不能说所有锁都是慢的.必须承认,很容易乱用锁,但不用太
担心,任何的瓶颈问题都会在细心的profiling中发现.当你考虑锁是如何的稳定,
相对容易的理解它们(与lock-free技术相比),锁有时候其实很好用.</p>

<h2 id="build-own-lightweight-mutex">Build own lightweight mutex</h2>

<p>我们也可以实现自己的简单轻量级的mutex,但仅仅作为教育手段,理解mutex一些
内在实现细节,实际现在操作系统都提供轻量级的mutex,千万不要自己实现一个
并实际使用,直接只用操作系统提供的即可.</p>

<p>网络上有很多种方法在用户层写自己的mutex:</p>

<ul>
  <li><a href="http://preshing.com/20120226/roll-your-own-lightweight-mutex/">roll-your-own-lightweight-mutex</a>利用Windows提供的semaphore和atomic操作实现的mutex.</li>
  <li><a href="http://cbloomrants.blogspot.hk/2011/07/07-15-11-review-of-many-mutex.html">Review of many Mutex implementations</a>很长的一篇文章,总结了很多种mutex的实现细节.</li>
</ul>

<p>这里利用
<a href="http://www.haiku-os.org/legacy-docs/benewsletter/Issue1-26.html#Engineering1-26">Benaphore</a>
技术,在Linux平台上利用<a href="http://pubs.opengroup.org/onlinepubs/9699919799/functions/sem_init.html">semaphore</a>和<a href="https://gcc.gnu.org/onlinedocs/gcc-4.1.2/gcc/Atomic-Builtins.html">atomic</a>操作实现自己的C++版本的
lightweight mutex.这里并没有用
<a href="http://www.open-std.org/JTC1/sc22/wg21/docs/papers/2007/n2427.html">C++11的原子库</a>.所
有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/benaphore_mutex">Github上</a>.</p>

<p>``` c++
 #include <semaphore.h>
class Benaphore {
 public:
  Benaphore() : counter_(0) {
    sem_init(&amp;semaphore_, 0, 0);
  }
  ~Benaphore() {
    sem_destroy(&amp;semaphore_);
  }
  void Lock() {
    if (__sync_add_and_fetch(&amp;counter_, 1) &gt; 1) {
      sem_wait(&amp;semaphore_);
    }
  }
  void Unlock() {
    if (__sync_sub_and_fetch(&amp;counter_, 1) &gt; 0) {
      sem_post(&amp;semaphore_);
    }
  }
  bool TryLock() {
    return __sync_bool_compare_and_swap(&amp;counter_, 0, 1);
  }</semaphore.h></p>

<p>private:
  long counter<em>;
  sem_t semaphore</em>;
};
```</p>

<p><a href="https://gcc.gnu.org/onlinedocs/gcc-4.1.2/gcc/Atomic-Builtins.html"><code>__sync_add_and_fetch</code></a>
是一个由GCC内部提供的 <em>atomic read-modify-write (RMW)</em> 操作,它把1加到
某个数并且返回新的数,在同一时间所有操作由一个线程原子操作完成,其他线程
不能干涉,只能在后等待.这里<code>counter_</code>初始化为0,第一个线程调用<code>Lock</code>将得
到1从<code>__sync_add_and_fetch</code>,然后跳过<code>sem_wait</code>,一旦这个线程占用这个锁,
之后线程都将递增<code>counter_</code>,获得大于1的数,从而调用<code>sem_wait</code>等待.</p>

<p>之后,第一个线程完成自己的操作,调用<code>Unlock</code>,<code>__sync_sub_and_fetch</code>的返
回值大于1说明有其他线程在等待这个mutex,调用<code>sem_post</code>唤醒其他线程.</p>

<h3 id="section">底层分析与性能</h3>

<p>上面使用了<code>__sync_add_and_fetch</code>,它编译成<code>lock xadd</code>指令如下.在没有竞
争下的lock/unlock操作性能与pthread mutex相当.但是在mutex多线程竞争情况
下,这个mutex性能没有pthread mutex好.</p>

<p><img src="/images/blog/2014/multithreading/lightweight_mutex_assembly.png" title="‘lightweight_mutex_assembly’" ></p>

<h3 id="mutex">增强Mutex支持递归</h3>

<p>上面简单的lightweight mutex的局限性是它不能递归.也就是同一个线程试图获
取同样的锁两次以上,将造成死锁(deadlock).递归锁在函数调用自己时很有用.比
如在内存管理代码中,可能会遇到如下代码:</p>

<p>``` c++
Realloc(void* ptr, size_t size)
{
    LOCK;</p>

<pre><code>if (ptr == NULL)
{
    return Alloc(size);
}
else if (size == 0)
{
    Free(size);
    return NULL;
}
else
    ... }
</code></pre>

<p>Alloc(size_t size)
{
    LOCK;</p>

<pre><code>... } ```
</code></pre>

<p><code>Lock</code>是个封装好的C++宏,用来获取锁和自动结果当退出函数.</p>

<p>可以看到,当传递<code>NULL</code>给<code>Realloc</code>,锁被<code>Realloc</code>函数获取,然后第二次被获
取当<code>Alloc</code>被调用.</p>

<p>把它扩展成可递归的锁如下,加入2个新成员变量,<code>owner_</code>,存储当前占有线程的
ID(TID),和<code>recursion_</code>,存储递归的层数.基本代码如下:</p>

<p>``` c++
 #include <semaphore.h>
 #include <pthread.h>
 #define LIGHT_ASSERT(x) { if (!(x)) __builtin_trap(); }</pthread.h></semaphore.h></p>

<p>class RecursiveBenaphore {
 public:
  RecursiveBenaphore() : counter<em>(0), owner</em>(0), recursion<em>(0) {
    sem_init(&amp;semaphore</em>, 0, 0);
  }
  ~RecursiveBenaphore() {
    sem_destroy(&amp;semaphore<em>);
  }
  void Lock() {
    pthread_t thread_id = pthread_self();
    if (__sync_add_and_fetch(&amp;counter</em>, 1) &gt; 1) {
      if (!pthread_equal(thread_id, owner<em>)) {
        sem_wait(&amp;semaphore</em>);
      }
    }
    owner_ = thread_id;
    recursion<em>++;
  }
  void Unlock() {
    pthread_t thread_id = pthread_self();
    LIGHT_ASSERT(pthread_equal(thread_id, owner</em>));
    long recur = –recursion<em>;
    if (recur == 0) {
      owner</em> = 0;
    }
    long result = <em>_sync_sub_and_fetch(&amp;counter</em>, 1);
    if (result &gt; 0) {
      if (recur == 0) {
        int sem_value;
        sem_getvalue(&amp;semaphore<em>, &amp;sem_value);
        if (sem_value == 0) {
          sem_post(&amp;semaphore</em>);
        }
      }
    }
  }
  bool TryLock() {
    pthread_t thread_id = pthread_self();
    if (pthread_equal(thread_id, owner<em>)) {
      __sync_add_and_fetch(&amp;counter</em>, 1);
    } else {
      bool result = <em>_sync_bool_compare_and_swap(&amp;counter</em>, 0, 1);
      if (result == false) {
        return false;
      }
      owner_ = thread_id;
    }
    recursion_++;
    return true;
  }</p>

<p>private:
  long counter<em>;
  sem_t semaphore</em>;
  pthread_t owner<em>;
  long recursion</em>;
};
```</p>

<p>如之前一样,第一个线程调用<code>Lock</code>,设置<code>owner_</code>为自己的TID,增加
<code>recursion_</code>到1.如果同一个线程再次调用<code>Lock</code>,它将同时增加
<code>recursion_</code>和<code>counter_</code>.</p>

<p>之后,第一个线程完成自己的操作,调用<code>Unlock</code>,同时减少<code>recursion_</code>和<code>counter_</code>,
仅仅调用<code>sem_post</code>唤醒其他线程当<code>recursion_</code>减少到<code>0</code>.如果
<code>recursion_</code>仍然大于0,意味着当前的线程仍然占有此锁在外层程序.</p>

<p>最后进行<strong>压力测试</strong>,建立一些线程,每个随机获取锁,随机的递归层次.代码在
<a href="https://github.com/shishougang/blog_multithreading/tree/master/benaphore_mutex">Github上</a>.</p>

<p>一些细节问题:
* 在<code>Unlock</code>中,设置<code>owner_</code>为0在调用<code>__sync_sub_and_fetch</code>之前,否则可
  能发生死锁(deadlock).比如,有两个线程TID是111和222.
    1. 线程111完成操作调用<code>Unlock</code>,先调用<code>__sync_sub_and_fetch</code>把<code>counter_</code>减到0
    2. 在设置<code>owner_</code>为0被中断,线程222得到运行,它调用<code>Lock</code>,发现<code>counter_</code>为0,跳过<code>sem_wait</code>,设置<code>owner_=222</code>,完成<code>Lock</code>操作.
    3. 线程222被中断调出,线程111重新得到运行,设置<code>owner_</code>为0,然后完成<code>Unlock</code>操作.
    4. 因为此时<code>owner_</code>为0,线程222不能在递归占用锁,一旦它再次获取锁,形成死锁.</p>

<ul>
  <li>
    <p>在<code>Unlock</code>中,<code>recursion_</code>被拷贝到本地变量一次,之后只本地变量,比如没
有在<code>__sync_sub_and_fetch</code>之后重新读取她.因为在那之后它能被其他线程
已经改变. </p>
  </li>
  <li>
    <p><code>recursion_</code>和<code>owner_</code>没有原子操作.因为它们在调用<code>Lock</code>的
<code>__sync_add_and_fetch</code>和调用<code>Unlock</code>的<code>__sync_sub_and_fetch</code>之间,线
程占有锁,独占<code>recursion_</code>和<code>owner_</code>的读写操作,并拥有所有的acquire
and release semantics.对<code>recursion_</code>和<code>owner_</code>使用原子操作没必要.因
为在X86/84的平台上,<code>__sync_add_and_fetch</code>生成<code>lock xadd</code>的指令,保证
全部的memory barrier,也就保证acquire and release semantics.</p>
  </li>
</ul>

<h2 id="mutex-vs-spinlock">Mutex VS Spinlock</h2>

<p>提到Mutex,往往会提到Spinlock,因为在使用Lock时,会遇到如何在Mutex与Spinlock之
间选择.那么接下来对比一下两者.</p>

<h3 id="section-1">定义</h3>

<p>Mutex: 如果一个线程试图获取一个mutex,但是没有成功,因为mutex已经被占用,
它将进入睡眠,让其他进程运行,直到mutex被其他进程释放.</p>

<p>Spinlock: 如果一个线程试图获取一个Spinlock, 但是没有成功,它将持续试着
去获取它,直到它最终成功获取,因为它将不允许其他线程运行(然而,操作系统将
强制调度其他线程).</p>

<h3 id="section-2">各自缺点</h3>

<p>Mutex: Mutex将使得线程睡眠,然后再唤醒它们,两者都是开销比较大的操作,也
就是context switch的开销.如果锁只是被其他线程占用非常短的时间,那么时间
花在使的线程睡眠并唤醒它可能超过它使用spinlock持续获取锁的时间.</p>

<p>Spinlock: Spinlock持续获取锁,浪费很多CPU时间,如果锁被其他线程占用很长
时间,那么它将浪费很多时间,不如使得线程进入睡眠,让出CPU.<a href="http://jfdube.wordpress.com/2011/09/24/lessons-learnt-while-spinning/">Spinlock的确能优化context switches</a>
但会在没有
<a href="http://en.wikipedia.org/wiki/Priority_inversion">threads priority inversion</a>
的平台上产生副作用.(但一个高优先级的线程自旋一个锁来等待一个低优先级的
线程释放这个锁,就会造成死锁).在没有Preemption的Uniprocessor,使用
spinlock是没有意义的,当前只有一个线程运行,没有必要保护关键区域,也没有其他线程同时运行,释放锁
给它.</p>

<p>所以在Linux下,Spinlock在kernel这样实现:</p>

<ul>
  <li>没有打开<code>CONFIG_SMP</code>和<code>CONFIG_PREEMPT</code>,spinlock实现代码是空的.</li>
  <li>没有打开<code>CONFIG_SMP</code>,打开<code>CONFIG_PREEMPT</code>,spinlock仅仅是简单的关闭
preemption,足够来防止任何的
<a href="http://en.wikipedia.org/wiki/Race_condition">races</a>. </li>
  <li>打开<code>CONFIG_SMP</code>,打开<code>CONFIG_PREEMPT</code>,spinlock实现如下代码,不断检查
lock是否被其他线程释放: </li>
</ul>

<p><code>c
  extern inline void spin_lock(spinlock_t *plock)
  {
    __asm__ __volatile__(
        spin_lock_string
        :"=m" (__dummy_lock(plock)));
  }
  // Macro spin_lock_string expand
  extern inline void spin_lock(spinlock_t *plock)
 {
  1:
    lock ; btsl ,plock;
    jc 2f;
    .section .text.lock,"ax"
  2: 
    testb ,plock;
    rep;nop;
    jne 2b;
    jmp 1b;
    .previous
 }
</code></p>

<h3 id="section-3">总结</h3>

<table>
  <thead>
    <tr>
      <th>Criteria</th>
      <th>Muutex</th>
      <th>Spinlock</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>机制</td>
      <td>尝试获取锁.若可得到就占有.若不能,就进入睡眠等待.</td>
      <td>尝试获取锁.若可得到就占有.若不能,持续尝试直到获取.</td>
    </tr>
    <tr>
      <td>什么时候使用</td>
      <td>当线程进入睡眠没有伤害.或需要等待一段足够长的时间才能获取锁.</td>
      <td>当线程不应该进入睡眠如中断处理等.当只需等待非常短的时间就能获取锁.</td>
    </tr>
    <tr>
      <td>缺点</td>
      <td>引起context switch和scheduling开销.</td>
      <td>线程不做任何事情在获取到锁前.浪费CPU运行.</td>
    </tr>
  </tbody>
</table>

<p><a href="http://en.wikipedia.org/wiki/Spinlock#Alternatives">大多数操作系统(包括Solaris,Mac OS X和FreeBSD)使用混合的机制叫”adaptive mutex”或”hybrid mutex”</a>.一
个hybrid mutex首先行为和spinlock一样,如果不能获取锁,持续尝试获取,但过
了一定的时间,它就和mutex一样,让线程进入睡眠.<sup id="fnref:f1"><a href="#fn:f1" rel="footnote">1</a></sup>.</p>

<div class="footnotes">
  <ol>
    <li id="fn:f1">
      <p>http://stackoverflow.com/questions/5869825/when-should-one-use-a-spinlock-instead-of-mutex<a href="#fnref:f1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
