<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Multithreading | (learn&think)]]></title>
  <link href="http://dreamrunner.org/tags/multithreading/atom.xml" rel="self"/>
  <link href="http://dreamrunner.org/"/>
  <updated>2014-07-06T23:38:58+08:00</updated>
  <id>http://dreamrunner.org/</id>
  <author>
    <name><![CDATA[DreamRunner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Multithreading相关术语总结]]></title>
    <link href="http://dreamrunner.org/blog/2014/07/05/multithreadingxiang-guan-zhu-yu-zong-jie/"/>
    <updated>2014-07-05T23:23:52+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/07/05/multithreadingxiang-guan-zhu-yu-zong-jie</id>
    <content type="html"><![CDATA[<p>在谈到内存模型,Multithreading,尤其lock-free programmming等时,总会遇到
一些相关术语来描述,如Memory Barrier,Acquire semantics,Release
semantics,happens-before relation等.在这里稍微整理一下.</p>

<!-- more -->

<h2 id="memory-barriers">Memory Barriers</h2>

<p>在之前
<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/">浅谈Memory Reordering</a>
中谈到编译器reordering和在多核下的处理器的reordering,在lock-free
programming中,如果不控制好这两者的reordering就会引起上文中所不想的结果.</p>

<p>你可以通过指令强制CPU和编译器在内存处理上的顺序,这些指令就被成为
<a href="http://en.wikipedia.org/wiki/Memory_barrier">Memory Barrier</a>.</p>

<p>有很多指令作为memory barriers,所以需要知道很多不同类型的memory
barriers. <a href="http://g.oswego.edu/dl/jmm/cookbook.html">Doug Lea指出</a>如下
的四大类可以很好的归纳在CPU上的特殊指令.尽管不是完全,大多数时候,一个正
真的CPU指令执行包含上面barrier类型的各种组合,或附带其他效果.无论如何,
一旦你理解了这四种类型的memory barriers,你就很好的理解了大部分真正CPU
的关于内存约束的指令.
<a href="http://preshing.com/20120710/memory-barriers-are-like-source-control-operations/">Memory Barriers Are Like Source Control Operations</a>
这篇把Memory Barriers与Source Control作类比,熟悉Source Control机制的可
以很形象的理解各类Memory Barriers机制.</p>

<p><img class="center" src="/images/blog/2014/multithreading/memory_barriers_types.png" title="‘memory_barriers_types’" ></p>

<h3 id="loadload">LoadLoad</h3>

<p>顺序: Load1; <strong>LoadLoad</strong>; Load2</p>

<p>保证Load1的数据加载在被load2和之后的load指令读取加载之前.是一个比较好
的方法防止看到旧的数据.以这个经典的例子,CPU1检查一个共享的标识变量flag来确
认一些数据是否被CPU1更新.如果标识变量flag是true的话,把<code>LoadLoad</code>barrier
放在读取更新数据之前:</p>

<p><code>c++
if (is_updated) {
    LOADLOAD_FENCE();  // Prevent reordering of loads
    return value;  // Load updated value
}
</code></p>

<p>只要<code>is_updated</code>被CPU1看到为true, <code>LoadLoad</code>fence防止CPU1读到比标识变
量flag本身旧的<code>value</code>.</p>

<h3 id="storestore">StoreStore</h3>

<p>顺序: Store1; <strong>StoreStore</strong>; Store2</p>

<p>保证Store1的数据被其他CPU看到在与这数据相关的Store2和之后的store指令之
前.同样,它足够的防止其他CPU看到自己的旧数据.同上一样的例子,CPU1需要更
新一些数据到共享的内存中,把<code>StoreStore</code> barrier放在标识变量flag是true
之前:</p>

<p><code>c++
value = x;
STORESTORE_FENCE();
is_updated = 1;  // Set shared flag to show the update of data
</code></p>

<p>一旦其他CPU看到<code>is_updated</code>为true,它能自信它看到正确的<code>value</code>值.而且
<code>value</code>不需要原子类型,它可以是一个包含很多元素的大数据结构.</p>

<h3 id="loadstore">LoadStore</h3>

<p>顺序: Load1; <strong>LoadStore</strong>; Store2</p>

<p>保证Load1的数据被加载在与这数据相关的Store2和之后的store指令之
前.</p>

<h3 id="storeload">StoreLoad</h3>

<p>顺序: Store1; <strong>StoreLoad</strong>; Load2</p>

<p>保证Store1的数据被其他CPU看到在数据被Load2和之后的load指令加载之前.也
就是说,它有效的防止所有barrier之前的stores与所有barrier之后的load乱序.</p>

<p><code>StoreLoad</code>是唯一的.它是唯一的memory barrier类型来防止<code>r1=r2=0</code>在之前
<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/#memory-ordering-at-processor-time">Memory ordering at processor time</a>
中给出的例子.</p>

<p><code>StoreLoad</code>有什么区别与<code>StoreStore</code>之后跟<code>LoadLoad</code>?虽
然,<code>StoreStore</code>按序把存储改变推送到主内存中,<code>LoadLoad</code>按序把改变加载过
来,但是这两种类型的barrier是不够的.Store可以延迟任意的指令,以致在Load
之后,Load也可以不是加载最新Store之后的内容.这就是为啥PowerPC的指令
<code>lwsync</code>,包含这三种memory barriers,<code>LoadLoad</code>,<code>LoadStore</code>和
<code>StoreStore</code>,但不包含<code>StoreLoad</code>,是不足以防止<code>r1=r2=0</code>在那个实例中.</p>

<h3 id="data-dependency-barriers">Data dependency barriers</h3>

<p>除了上面4大类,还有<code>Loadload</code>的弱化模式的<code>Data dependency barrier</code>.如
<code>LoadLoad</code>类似,在两个load顺序执行,load2依赖于load1的结果,<code>Data
dependency barrier</code>需要插入保证两者的顺序.</p>

<p>但与<code>LoadLoad</code>不同,<code>Data dependency barrier</code>只是部分顺序约束在内在以来
的load,就是load1必须与load2是 <strong>data</strong> dependency 而不是仅仅是
<strong>control</strong> dependency.</p>

<ul>
  <li>data dependency</li>
</ul>

<p>r1与r2之间是data dependency.</p>

<p><code>c
r1 = 1;
r2 = r1;
</code></p>

<ul>
  <li>control dependency</li>
</ul>

<p>r1与r2之间是control dependency.</p>

<p><code>c
r1 = value;
if (r1) {
    r2 = r1;
} else {
    r2 = 1;
}
</code></p>

<h3 id="more">More</h3>

<ul>
  <li><a href="https://www.kernel.org/doc/Documentation/memory-barriers.txt">LINUX KERNEL MEMORY BARRIERS</a> </li>
  <li><a href="www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf">Memory Barriers: a Hardware View for Software Hackers</a></li>
</ul>

<h2 id="acquire-and-release-semantics">Acquire and Release semantics</h2>

<p>在lock-free programming中,共享内存被多个线程通过合作传递信息来处理,在
这种处理下,acquire和release semantics是关键技术保证可靠的传递信息在线
程之间.</p>

<p>acqure和release semantics并没有好的被定义,这里借用Jeff Preshing在
<a href="http://preshing.com/20120913/acquire-and-release-semantics/">这里</a>给
予的定义:</p>

<p><img class="right" src="/images/blog/2014/multithreading/read_acquire.png" width="170" height="110" title="‘read_acquire’" ></p>

<p><strong>Acquire semantics</strong> 是一种只能应用于如下操作的性质: 从
共享内存读取,无论是
<a href="http://en.wikipedia.org/wiki/Read-modify-write">read-modify-write</a>操
作还是普通的加载.这一操作被认为是一个 <strong>read acquire</strong>. Acquire
semantics防止read acquire程序上<strong>之后</strong>的任何读或写操作与它的内存乱
序.</p>

<p><br /></p>

<p><img class="right" src="/images/blog/2014/multithreading/write_release.png" width="170" height="110" title="‘write_release’" ></p>

<p><strong>Release semantics</strong> 是一种只能应用于如下操作的性质: 写入到共享内存,
无论是read-modify-write操作还是普通的存储.这一操作被认为是一个 <strong>write release</strong>.
Release semantics防止write release程序上<strong>之前</strong>的任何读或写
操作与它的乱序.</p>

<p>Acqure和release semantics能通过之前四种memory barrier的简单组合来达到.</p>

<p><img class="center" src="/images/blog/2014/multithreading/acquire_release_semantics.png" title="‘acquire_release_semantics’" ></p>

<p>Acqure和release semantics可以基本划分为如下结构:</p>

<p><img class="center" src="/images/blog/2014/multithreading/acquire_release_semantics_category.png" title="‘acquire_release_semantics_category’" ></p>

<h3 id="fence">使用明确的平台相关Fence指令</h3>

<p>在X86/64使用<code>mefence</code>指令,mfence是一个满足全部memory barrier,防止任何类型的内存乱序.</p>

<p><img class="center" src="/images/blog/2014/multithreading/platform-specific_fence.png" title="‘platform-specific_fence’" ></p>

<h3 id="c11fences">可移植的C++11的Fences</h3>

<p>C++11的atomic库定义了一个可移植的函数<code>atomic_thread_fence()</code>,输入一个
变量来指定什么类型的fence.</p>

<p><img class="center" src="/images/blog/2014/multithreading/fence_in_c++11.png" title="‘fence_in_c++11’" ></p>

<h3 id="c11atomicfence">可移植的C++11的atomic,非明确的fence</h3>

<p>在C++11中,可以直接对atomic变量直接约束fence,而不是显示的明确fence.与上
面明确fence相比,这实际是更优的方法来表达acquire and release semantics
在C++11中.</p>

<p><img class="center" src="/images/blog/2014/multithreading/without_fence_c++11.png" title="‘without_fence_c++11’" ></p>

<h2 id="happens-before-relation">Happens-before relation</h2>

<p><em>Happens-before</em> 是一个术语来描述C++11,Java,LLVM之类背后的<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/#weak-vs-strong-memory-models">软件内存模型</a>.</p>

<p>在之上每个语言里都能找到<em>happends-before</em>的定义,尽管每个都有不同的说法,但
内在意思基本一致.粗略地讲,基本定义如下:</p>

<p><blockquote><p>A和B表示一个多线程进行的操作.若A <strong>happens-before</strong> B,那<br/>么,在B进行前,A对B的内存影响有效的被B看到.</p></blockquote></p>

<p>无论使用任何编程语言,它们都有一个共同处:如果操作A和B被同一个进程进行,A
的语句在B的语句之前在程序顺序上,那么A<em>优先发生(happens-before)</em>B.这也
是在之前
<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/#weak-vs-strong-memory-models">Memory ordering</a>
中谈到中心原则.</p>

<p>这里再次提一下指令重排序问题,有人有如下疑问: 指令重排序会破坏
happens-before原则吗？happens-before的程序次序原则说：在一个线程内，按
照程序代码顺序，书写在前面的操作会先行发生于书写在后面的操作。如果线程
内出现指令重排序，那不是破坏了程序次序原则了吗？</p>

<p>是会破坏程序次序的执行,但是并不破坏happens-before原则,并不造成内存对单
线程有效性的破坏.这里主要的困惑是时间上顺序的发生之前(happening
before)与先行发生(happens-before)两者关系.</p>

<p>时间上顺序的发生在前于(happening before)与先行发生(happens-before)两者是
不一样的,基本没太大关系.特别:</p>

<ol>
  <li>A先行发生(happens-before)B并不意味着A发生在前于(happening before)B.</li>
  <li>A发生在前于(happening before)B并不意味A先行发生(happens-before)B.</li>
</ol>

<p>谨记happens-before是由一系列编程语言特定定义的操作间的关系,它的存在独
立于时间的概念.</p>

<h3 id="happens-beforehappening-before">happens-before并不意味happening before</h3>

<p>如下例子有happens-before关系但并不是顺序执行,没有happening before.如下
代码:(1) 存储到A,之后(2)存储到B.根据程序顺序原则,(1) happens-before (2).</p>

<p><code>c++
int A, B;
void test() {
  A = B + 1;  // (1)
  B = 0;  // (2)
}
</code></p>

<p>用O2打开优化编译的如下:</p>

<p>``` sh
$ gcc -S -O2  -masm=intel test.c</p>

<pre><code>mov	eax, DWORD PTR B
mov	DWORD PTR B, 0
add	eax, 1
mov	DWORD PTR A, eax ```
</code></pre>

<p>从汇编指令看出,第二句<code>mov DWORD PTR B, 0</code>就已经完成对<code>B</code>的存储,但是
对<code>A</code>的存储还没进行.(1)顺序上并没有在(2)之前执行!</p>

<p>但是happens-before原则有被违背吗?根据定义,(1)的内存效用必须有效被看到
在进行(2)之前.也就是存储A必须影响存储B.</p>

<p>在这里,存储A实际并没有影响存储B.(2)被提前执行与之后执行仍然一样,相当与
(1)的内存有效性是一样的.因此,这并不算违背happens-before原则.</p>

<h3 id="happening-beforehappens-before">happening before并不意味happens-before</h3>

<p>这是个时间上发生于前但并含有happens-before关系的例子.如下的代码,想象一
个线程调用<code>UpdateValue</code>,而另一个线程调用<code>ConsumeValue</code>.因为处理共享的
数据并行的,为了简单,认为普通的读取和存储<code>int</code>是atomic的.因为程序顺序原
则,在(1)和(2)之间happens-before关系,(3)和(4)之间happens-before关系.</p>

<p>``` c++
int value = 0;
int updated = 0;</p>

<p>void UpdateValue() {
    value = 123;  // (1)
    update = 1;  // (2)
}</p>

<p>void ConsumeValue() {
if (update) {  // (3)
    printf(“%d\n”, value);  // (4)
}
```</p>

<p>进一步假设在运行开始的时候,(3)读取<code>update</code>到为1,这个值是有(2)在另外个线程
中存储的.这里,我们可以得出时间顺序上(2)必须发生前于(3).但是这里并没有规
则意味着在(2)和(3)之间有happens-before关系.(2)和(3)之间没有
happens-before关系,(1)和(4)之间也没有happens-before关系.因此,(1)和(4)
的内存可以重排序,因为编译器重排序或在CPU上内存重排序,以致(4)可以打印
“0”,即使(3)读到1.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[浅谈Mutex (Lock)]]></title>
    <link href="http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock/"/>
    <updated>2014-06-29T20:52:09+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock</id>
    <content type="html"><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Mutual_exclusion">Mutex</a>(又叫Lock),在多线程中,作为同步的基本类型,用来保证没有两个线程或进程同时在他们的关键区域.因为Mutex这种排它性,很多人认为Mutex开销很大,尽量避免使用它.就如这篇
分析完共享数据问题后,进一步分析说明
<a href="http://courses.cs.washington.edu/courses/cse451/03wi/section/prodcons.htm">Avoiding locks</a>
来解决这个问题.但Mutex真的开销如此大,还是被大家误解了?Matthew
Dillon<a href="http://groups.google.com/group/net.micro.mac/msg/752d18de371bd65c?dmode=source">写道</a>,”Most
people have the misconception that locks are slow.”, Jeff Preshing也
<a href="http://preshing.com/20111118/locks-arent-slow-lock-contention-is/">写了这篇”Locks Aren’t Slow; Lock Contention Is”</a>.</p>

<p>那么接下来做3个关于Mutex的Benchmark,具体分析一下Mutex的开销如何,最后并
利用原子操作和semaphore实现一个lightweight Mutex.</p>

<!-- more -->

<p>一个Mutex仅仅从Lock到Unlock具体开销是多少,是不是占用很多时间,从
<a href="http://preshing.com/20111124/always-use-a-lightweight-mutex/">Always Use a Lightweight Mutex</a>
从可以看到在windows中有两种
Mutex:<a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms684266%28v=vs.85%29.aspx">Muetx</a>
和
<a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms682530%28v=vs.85%29.aspx">Critical Section</a>,
重量级和轻量级的区别,两者的时间开销相差25倍多,所以一直使用轻量级的Mutex.</p>

<p><a href="http://ridiculousfish.com/blog/posts/barrier.html">这篇文章</a>在高强度
下lock的性能:每个线程做任何事情都占用lock(高冲突),lock占用极短的时间
(高频率).值得一读,但是在实际应用中,基本避免如此使用locks.这里对
Mutex Contention和Mutex Frequency都做最好和最坏场景的使用测试.</p>

<p>Mutex被灌以避免使用也因为其他原因.现在有很多大家熟知的
<a href="en.wikipedia.org/wiki/Non-blocking_algorithma">lock-free programming</a>
技术.Lock-free编程非常具有挑战性,但在实际场景中获得巨大的性能.既然有
lock-free的技术吸引我们使用它们,那么locks就显得索然无味了.</p>

<p>但也不能因此忽略lock.因为在实际很多场景,它仍然是利器.</p>

<h2 id="lightweight-mutex-benchmark">Lightweight Mutex Benchmark</h2>

<p>Linux下的POSIX thread是轻量级的Mutex.基于Linux特有的
<a href="http://en.wikipedia.org/wiki/Futex">futex</a>技术,当没有其他线程竞争锁时它被优化过.使
用如下简单的例子,测试一个单线程lock和unlock,所有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/mutex_time">Github上</a>.</p>

<p><code>c++
pthread_mutex_init(&amp;lock, NULL);
const int kN = 1000000;
for (int i = 0; i &lt; kN; ++i) {
    pthread_mutex_lock(&amp;lock);
    pthread_mutex_unlock(&amp;lock);
}
pthread_mutex_destroy(&amp;lock);
</code></p>

<p>插入相应的时间代码,算出10万次的单线程lock/unlock平均时间.在不同的处理
器下,结果如下:</p>

<p><img class="center" src="/images/blog/2014/multithreading/mutex_benchmark.png" width="450" height="200" title="‘mutex_benchmark’" ></p>

<p>如果假设一个线程每分钟获取1e5次mutex,并且没有其他线程与它竞争.基于如下
的图,可预计0.2%到0.4%的开销.不算差.在比较低频率下,开销基本忽略不计.之
后<a href="http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock#build-own-lightweight-mutex">Build own lightweight mutex</a>,会利用<a href="http://en.wikipedia.org/wiki/Semaphore_%28programming%29">semaphore</a>和一个原子操作,实现一个lightweight mutex.</p>

<p>POSIX thread与Windows Critical Section不同,它不仅支持线程间的同步,
还支持进程间的同步.实例代码如下:</p>

<p>```  c++  mutex_between_process.cc
pthread_mutex_t mutex;
pthread_mutexattr_t attrmutex;</p>

<p>/* Initialise attribute to mutex. */
pthread_mutexattr_init(&amp;attrmutex);
pthread_mutexattr_setpshared(&amp;attrmutex, PTHREAD_PROCESS_SHARED);
pthread_mutex_init(&amp;mutex, &amp;attrmutex);</p>

<p>/* Use the mutex. */</p>

<p>/* Clean up. */
pthread_mutex_destroy(pmutex);
pthread_mutexattr_destroy(&amp;attrmutex);
```</p>

<h2 id="mutex-contention-benchmark">Mutex Contention Benchmark</h2>

<p>在测试中,产生一个不断生成随机数的线程,使用自己编制的线程安全的
<a href="http://en.wikipedia.org/wiki/Mersenne_twister">Mersenne Twister</a>实现
代码.每过一段时间,它获取和释放一个锁,获取和释放锁之间的时间每次是随机的,但
是总的平均时间是提前设计好的.这个随机的过程就是个泊松分布过程,计算出产
生一个随机数的平均时间6.25 ns在2.93 GHz i7上,把它作为运行单位.利用
<a href="http://wiki.dreamrunner.org/public_html/Algorithms/Theory%20of%20Algorithms/poisson-process.html">Poisson Process</a>
的算法决定运行多少个运行单位在获取和释放锁之间.并利用
<a href="http://dreamrunner.org/blog/2014/06/24/high-resolution-time/">High Resolution Time</a>API
计算时间.这个线程的代码如下,所有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/mutex_contention">Github上</a>:</p>

<p>``` c++
  GetMonotonicTime(&amp;start);
  for (;;) {
    work_units = static_cast<int> (random.PoissonInterval(
        global_state.average_unlock_count) + 0.5f);
    for (int i = 0; i &lt; work_units; ++i) {
      random.Integer();
    }
    thread_stats.workdone += work_units;</int></p>

<pre><code>GetMonotonicTime(&amp;end);
elapsed_time = GetElapsedTime(&amp;start, &amp;end);
if (elapsed_time &gt;= global_state.time_limit) {
  break;
}

// Do some work while holding the lock
pthread_mutex_lock(&amp;global_state.thread_mutex);
work_units = static_cast&lt;int&gt; (random.PoissonInterval(
    global_state.average_locked_count) + 0.5f);
for (int i = 0; i &lt; work_units; ++i) {
  random.Integer();
}
thread_stats.workdone += work_units;
pthread_mutex_unlock(&amp;global_state.thread_mutex);

thread_stats.iterations++;
GetMonotonicTime(&amp;end);
elapsed_time = GetElapsedTime(&amp;start, &amp;end);
if (elapsed_time &gt;= global_state.time_limit) {
  break;
}   } ```
</code></pre>

<p>这里模拟获取和释放15000次锁每秒,从1个线程运行到2个线程,最后到4个线
程.并且验证占用锁的时间,从0%到100%的每次运行时间占用锁.把1个线程的完成
的工作量作为基准数据,其他的去除以它,计算相对增益.基本测试方案如下:</p>

<p><code>c++
// Test 15000 locks per second: thread number, lock_interval
    1, 1/15000.0f, 
    2, 1/15000.0f,
    3, 1/15000.0f,
    4, 1/15000.0f,
</code></p>

<p><img src="/images/blog/2014/multithreading/lock_benchmark.png" title="lock_benchmark’" ></p>

<p>从图中看出,随着锁占用的时间增加,并行性越来越差,直到最后占用60%以后,单
线程运行的更好.可以说,短时间的占用锁的时间,以10%以内,系统达到很高的并
行性.虽然并不是完美的,但是也接近.锁总体很快.</p>

<p>把这个结果放到实际中,Jeff Preshing在
<a href="http://preshing.com/20111118/locks-arent-slow-lock-contention-is/">这篇</a>
提到,实际的游戏程序中,15000的锁每秒来自3个线程,占用锁的时间相对2%.在图
中很适中的区域.</p>

<h2 id="mutex-frequency-benchmark">Mutex Frequency Benchmark</h2>

<p>尽管一个lightweight mutex有开销,但如上测试在2.40GHz i5上,lock/unlock锁
开销约 <strong>34.2ns</strong> ,因此15000锁每秒开销很低以致不是严重影响结果.那么把
锁的每秒频率提高呢?</p>

<p>只创建2个线程,进行一系列的锁的每秒频率测试在2.40GHz i5上,从占用锁时间
10 ns(1e8/s)到100 us(1e4/s),用单线程的占用锁时间10 ms作为基准工作量,其
他与它比较,测试方案如下:</p>

<p>``` c++
  // Reference
  1, 10e-3f,      // 10 ms        100/s</p>

<pre><code>// Test various lock rates with 2 threads
2, 10e-9f,      // 10 ns        100000000/s
2, 31.6e-9f,    // 31.6 ns      31600000/s
2, 100e-9f,     // 100 ns       10000000/s
2, 316e-9f,     // 316 ns       3160000/s
2, 1e-6f,       // 1 us         1000000/s
2, 3.16e-6f,    // 3.16 us      316000/s
2, 10e-6f,      // 10 us        100000/s
2, 31.6e-6f,    // 31.6 us      31600/s
2, 100e-6f,     // 100 us       10000/s ```
</code></pre>

<p><img src="/images/blog/2014/multithreading/frequency_benchmark.png" title="‘frequency_bechmark’" ></p>

<p>如预想一样,对于非常高频率的锁,锁的开销开始减少实际工作量.在网络上,可以
找到很多同样的测试.图中下边的线条,对于这样高的频率,也就是占用锁的时间
很短,就一些CPU的指令,这样的情况下,当锁之间的工作如此简单,那么一个
lock-free的实现更适合.</p>

<p>我们获得了一大块关于锁的性能:从它进行很好的情况,到缓慢应用的情况.在考
虑实际锁的使用情况,不能说所有锁都是慢的.必须承认,很容易乱用锁,但不用太
担心,任何的瓶颈问题都会在细心的profiling中发现.当你考虑锁是如何的稳定,
相对容易的理解它们(与lock-free技术相比),锁有时候其实很好用.</p>

<h2 id="build-own-lightweight-mutex">Build own lightweight mutex</h2>

<p>我们也可以实现自己的简单轻量级的mutex,但仅仅作为教育手段,理解mutex一些
内在实现细节,实际现在操作系统都提供轻量级的mutex,千万不要自己实现一个
并实际使用,直接只用操作系统提供的即可.</p>

<p>网络上有很多种方法在用户层写自己的mutex:</p>

<ul>
  <li><a href="http://preshing.com/20120226/roll-your-own-lightweight-mutex/">roll-your-own-lightweight-mutex</a>利用Windows提供的semaphore和atomic操作实现的mutex.</li>
  <li><a href="http://cbloomrants.blogspot.hk/2011/07/07-15-11-review-of-many-mutex.html">Review of many Mutex implementations</a>很长的一篇文章,总结了很多种mutex的实现细节.</li>
</ul>

<p>这里利用
<a href="http://www.haiku-os.org/legacy-docs/benewsletter/Issue1-26.html#Engineering1-26">Benaphore</a>
技术,在Linux平台上利用<a href="http://pubs.opengroup.org/onlinepubs/9699919799/functions/sem_init.html">semaphore</a>和<a href="https://gcc.gnu.org/onlinedocs/gcc-4.1.2/gcc/Atomic-Builtins.html">atomic</a>操作实现自己的C++版本的
lightweight mutex.这里并没有用
<a href="http://www.open-std.org/JTC1/sc22/wg21/docs/papers/2007/n2427.html">C++11的原子库</a>.所
有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/benaphore_mutex">Github上</a>.</p>

<p>``` c++
 #include <semaphore.h>
class Benaphore {
 public:
  Benaphore() : counter_(0) {
    sem_init(&amp;semaphore_, 0, 0);
  }
  ~Benaphore() {
    sem_destroy(&amp;semaphore_);
  }
  void Lock() {
    if (__sync_add_and_fetch(&amp;counter_, 1) &gt; 1) {
      sem_wait(&amp;semaphore_);
    }
  }
  void Unlock() {
    if (__sync_sub_and_fetch(&amp;counter_, 1) &gt; 0) {
      sem_post(&amp;semaphore_);
    }
  }
  bool TryLock() {
    return __sync_bool_compare_and_swap(&amp;counter_, 0, 1);
  }</semaphore.h></p>

<p>private:
  long counter<em>;
  sem_t semaphore</em>;
};
```</p>

<p><a href="https://gcc.gnu.org/onlinedocs/gcc-4.1.2/gcc/Atomic-Builtins.html"><code>__sync_add_and_fetch</code></a>
是一个由GCC内部提供的 <em>atomic read-modify-write (RMW)</em> 操作,它把1加到
某个数并且返回新的数,在同一时间所有操作由一个线程原子操作完成,其他线程
不能干涉,只能在后等待.这里<code>counter_</code>初始化为0,第一个线程调用<code>Lock</code>将得
到1从<code>__sync_add_and_fetch</code>,然后跳过<code>sem_wait</code>,一旦这个线程占用这个锁,
之后线程都将递增<code>counter_</code>,获得大于1的数,从而调用<code>sem_wait</code>等待.</p>

<p>之后,第一个线程完成自己的操作,调用<code>Unlock</code>,<code>__sync_sub_and_fetch</code>的返
回值大于1说明有其他线程在等待这个mutex,调用<code>sem_post</code>唤醒其他线程.</p>

<h3 id="section">底层分析与性能</h3>

<p>上面使用了<code>__sync_add_and_fetch</code>,它编译成<code>lock xadd</code>指令如下.在没有竞
争下的lock/unlock操作性能与pthread mutex相当.但是在mutex多线程竞争情况
下,这个mutex性能没有pthread mutex好.</p>

<p><img src="/images/blog/2014/multithreading/lightweight_mutex_assembly.png" title="‘lightweight_mutex_assembly’" ></p>

<h3 id="mutex">增强Mutex支持递归</h3>

<p>上面简单的lightweight mutex的局限性是它不能递归.也就是同一个线程试图获
取同样的锁两次以上,将造成死锁(deadlock).递归锁在函数调用自己时很有用.比
如在内存管理代码中,可能会遇到如下代码:</p>

<p>``` c++
Realloc(void* ptr, size_t size)
{
    LOCK;</p>

<pre><code>if (ptr == NULL)
{
    return Alloc(size);
}
else if (size == 0)
{
    Free(size);
    return NULL;
}
else
    ... }
</code></pre>

<p>Alloc(size_t size)
{
    LOCK;</p>

<pre><code>... } ```
</code></pre>

<p><code>Lock</code>是个封装好的C++宏,用来获取锁和自动结果当退出函数.</p>

<p>可以看到,当传递<code>NULL</code>给<code>Realloc</code>,锁被<code>Realloc</code>函数获取,然后第二次被获
取当<code>Alloc</code>被调用.</p>

<p>把它扩展成可递归的锁如下,加入2个新成员变量,<code>owner_</code>,存储当前占有线程的
ID(TID),和<code>recursion_</code>,存储递归的层数.基本代码如下:</p>

<p>``` c++
 #include <semaphore.h>
 #include <pthread.h>
 #define LIGHT_ASSERT(x) { if (!(x)) __builtin_trap(); }</pthread.h></semaphore.h></p>

<p>class RecursiveBenaphore {
 public:
  RecursiveBenaphore() : counter<em>(0), owner</em>(0), recursion<em>(0) {
    sem_init(&amp;semaphore</em>, 0, 0);
  }
  ~RecursiveBenaphore() {
    sem_destroy(&amp;semaphore<em>);
  }
  void Lock() {
    pthread_t thread_id = pthread_self();
    if (__sync_add_and_fetch(&amp;counter</em>, 1) &gt; 1) {
      if (!pthread_equal(thread_id, owner<em>)) {
        sem_wait(&amp;semaphore</em>);
      }
    }
    owner_ = thread_id;
    recursion<em>++;
  }
  void Unlock() {
    pthread_t thread_id = pthread_self();
    LIGHT_ASSERT(pthread_equal(thread_id, owner</em>));
    long recur = –recursion<em>;
    if (recur == 0) {
      owner</em> = 0;
    }
    long result = <em>_sync_sub_and_fetch(&amp;counter</em>, 1);
    if (result &gt; 0) {
      if (recur == 0) {
        int sem_value;
        sem_getvalue(&amp;semaphore<em>, &amp;sem_value);
        if (sem_value == 0) {
          sem_post(&amp;semaphore</em>);
        }
      }
    }
  }
  bool TryLock() {
    pthread_t thread_id = pthread_self();
    if (pthread_equal(thread_id, owner<em>)) {
      __sync_add_and_fetch(&amp;counter</em>, 1);
    } else {
      bool result = <em>_sync_bool_compare_and_swap(&amp;counter</em>, 0, 1);
      if (result == false) {
        return false;
      }
      owner_ = thread_id;
    }
    recursion_++;
    return true;
  }</p>

<p>private:
  long counter<em>;
  sem_t semaphore</em>;
  pthread_t owner<em>;
  long recursion</em>;
};
```</p>

<p>如之前一样,第一个线程调用<code>Lock</code>,设置<code>owner_</code>为自己的TID,增加
<code>recursion_</code>到1.如果同一个线程再次调用<code>Lock</code>,它将同时增加
<code>recursion_</code>和<code>counter_</code>.</p>

<p>之后,第一个线程完成自己的操作,调用<code>Unlock</code>,同时减少<code>recursion_</code>和<code>counter_</code>,
仅仅调用<code>sem_post</code>唤醒其他线程当<code>recursion_</code>减少到<code>0</code>.如果
<code>recursion_</code>仍然大于0,意味着当前的线程仍然占有此锁在外层程序.</p>

<p>最后进行<strong>压力测试</strong>,建立一些线程,每个随机获取锁,随机的递归层次.代码在
<a href="https://github.com/shishougang/blog_multithreading/tree/master/benaphore_mutex">Github上</a>.</p>

<p>一些细节问题:
* 在<code>Unlock</code>中,设置<code>owner_</code>为0在调用<code>__sync_sub_and_fetch</code>之前,否则可
  能发生死锁(deadlock).比如,有两个线程TID是111和222.
    1. 线程111完成操作调用<code>Unlock</code>,先调用<code>__sync_sub_and_fetch</code>把<code>counter_</code>减到0
    2. 在设置<code>owner_</code>为0被中断,线程222得到运行,它调用<code>Lock</code>,发现<code>counter_</code>为0,跳过<code>sem_wait</code>,设置<code>owner_=222</code>,完成<code>Lock</code>操作.
    3. 线程222被中断调出,线程111重新得到运行,设置<code>owner_</code>为0,然后完成<code>Unlock</code>操作.
    4. 因为此时<code>owner_</code>为0,线程222不能在递归占用锁,一旦它再次获取锁,形成死锁.</p>

<ul>
  <li>
    <p>在<code>Unlock</code>中,<code>recursion_</code>被拷贝到本地变量一次,之后只本地变量,比如没
有在<code>__sync_sub_and_fetch</code>之后重新读取她.因为在那之后它能被其他线程
已经改变. </p>
  </li>
  <li>
    <p><code>recursion_</code>和<code>owner_</code>没有原子操作.因为它们在调用<code>Lock</code>的
<code>__sync_add_and_fetch</code>和调用<code>Unlock</code>的<code>__sync_sub_and_fetch</code>之间,线
程占有锁,独占<code>recursion_</code>和<code>owner_</code>的读写操作,并拥有所有的acquire
and release semantics.对<code>recursion_</code>和<code>owner_</code>使用原子操作没必要.因
为在X86/84的平台上,<code>__sync_add_and_fetch</code>生成<code>lock xadd</code>的指令,保证
全部的memory barrier,也就保证acquire and release semantics.</p>
  </li>
</ul>

<h2 id="mutex-vs-spinlock">Mutex VS Spinlock</h2>

<p>提到Mutex,往往会提到Spinlock,因为在使用Lock时,会遇到如何在Mutex与Spinlock之
间选择.那么接下来对比一下两者.</p>

<h3 id="section-1">定义</h3>

<p>Mutex: 如果一个线程试图获取一个mutex,但是没有成功,因为mutex已经被占用,
它将进入睡眠,让其他进程运行,直到mutex被其他进程释放.</p>

<p>Spinlock: 如果一个线程试图获取一个Spinlock, 但是没有成功,它将持续试着
去获取它,直到它最终成功获取,因为它将不允许其他线程运行(然而,操作系统将
强制调度其他线程).</p>

<h3 id="section-2">各自缺点</h3>

<p>Mutex: Mutex将使得线程睡眠,然后再唤醒它们,两者都是开销比较大的操作,也
就是context switch的开销.如果锁只是被其他线程占用非常短的时间,那么时间
花在使的线程睡眠并唤醒它可能超过它使用spinlock持续获取锁的时间.</p>

<p>Spinlock: Spinlock持续获取锁,浪费很多CPU时间,如果锁被其他线程占用很长
时间,那么它将浪费很多时间,不如使得线程进入睡眠,让出CPU.<a href="http://jfdube.wordpress.com/2011/09/24/lessons-learnt-while-spinning/">Spinlock的确能优化context switches</a>
但会在没有
<a href="http://en.wikipedia.org/wiki/Priority_inversion">threads priority inversion</a>
的平台上产生副作用.(但一个高优先级的线程自旋一个锁来等待一个低优先级的
线程释放这个锁,就会造成死锁).在没有Preemption的Uniprocessor,使用
spinlock是没有意义的,当前只有一个线程运行,没有必要保护关键区域,也没有其他线程同时运行,释放锁
给它.</p>

<p>所以在Linux下,Spinlock在kernel这样实现:</p>

<ul>
  <li>没有打开<code>CONFIG_SMP</code>和<code>CONFIG_PREEMPT</code>,spinlock实现代码是空的.</li>
  <li>没有打开<code>CONFIG_SMP</code>,打开<code>CONFIG_PREEMPT</code>,spinlock仅仅是简单的关闭
preemption,足够来防止任何的
<a href="http://en.wikipedia.org/wiki/Race_condition">races</a>. </li>
  <li>打开<code>CONFIG_SMP</code>,打开<code>CONFIG_PREEMPT</code>,spinlock实现如下代码,不断检查
lock是否被其他线程释放: </li>
</ul>

<p><code>c
  extern inline void spin_lock(spinlock_t *plock)
  {
    __asm__ __volatile__(
        spin_lock_string
        :"=m" (__dummy_lock(plock)));
  }
  // Macro spin_lock_string expand
  extern inline void spin_lock(spinlock_t *plock)
 {
  1:
    lock ; btsl ,plock;
    jc 2f;
    .section .text.lock,"ax"
  2: 
    testb ,plock;
    rep;nop;
    jne 2b;
    jmp 1b;
    .previous
 }
</code></p>

<h3 id="section-3">总结</h3>

<table>
  <thead>
    <tr>
      <th>Criteria</th>
      <th>Muutex</th>
      <th>Spinlock</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>机制</td>
      <td>尝试获取锁.若可得到就占有.若不能,就进入睡眠等待.</td>
      <td>尝试获取锁.若可得到就占有.若不能,持续尝试直到获取.</td>
    </tr>
    <tr>
      <td>什么时候使用</td>
      <td>当线程进入睡眠没有伤害.或需要等待一段足够长的时间才能获取锁.</td>
      <td>当线程不应该进入睡眠如中断处理等.当只需等待非常短的时间就能获取锁.</td>
    </tr>
    <tr>
      <td>缺点</td>
      <td>引起context switch和scheduling开销.</td>
      <td>线程不做任何事情在获取到锁前.浪费CPU运行.</td>
    </tr>
  </tbody>
</table>

<p><a href="http://en.wikipedia.org/wiki/Spinlock#Alternatives">大多数操作系统(包括Solaris,Mac OS X和FreeBSD)使用混合的机制叫”adaptive mutex”或”hybrid mutex”</a>.一
个hybrid mutex首先行为和spinlock一样,如果不能获取锁,持续尝试获取,但过
了一定的时间,它就和mutex一样,让线程进入睡眠.<sup id="fnref:f1"><a href="#fn:f1" rel="footnote">1</a></sup>.</p>

<div class="footnotes">
  <ol>
    <li id="fn:f1">
      <p>http://stackoverflow.com/questions/5869825/when-should-one-use-a-spinlock-instead-of-mutex<a href="#fnref:f1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[浅谈Memory Reordering]]></title>
    <link href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/"/>
    <updated>2014-06-28T22:55:22+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering</id>
    <content type="html"><![CDATA[<h2 id="memory-ordering">Memory ordering</h2>
<p>在我们编写的C/C++代码和它被在CPU上运行,按照一些规则,代码的内存交互会被
乱序.内存乱序同时由编译器(编译时候)和处理器(运行时)造成,都为了使代码运
行的更快.</p>

<p><img src="/images/blog/2014/multithreading/memory_model.png" title="‘memory_ordering’" ></p>

<p>被编译开发者和处理器制造商遵循的中心内存排序准则是:
<blockquote><p>不能改变单线程程序的行为.</p></blockquote></p>

<p>因为这条规则,在写单线程代码时内存乱序被普遍忽略.即使在多线程程序中,它
也被时常忽略,因为有mutexes,semaphores等来防止它们调用中的内存乱序.仅当
lock-free技术被使用时,内存在不受任何互斥保护下被多个线程共享,内存乱序
的影响能被看到.</p>

<p>下面先比较Weak和Strong的内存模型,然后分两部分,实际内存乱序如何在编译和运行时发生,并如何防止它们.</p>

<!-- more -->

<h2 id="weak-vs-strong-memory-models">Weak VS strong Memory Models</h2>
<p><a href="http://preshing.com/about">Jeff Preshing</a>在
<a href="http://preshing.com/20120930/weak-vs-strong-memory-models/">Weak vs. Strong Memory Models</a>
中很好的总结了从Weak到Strong的类型:</p>

<table>
  <thead>
    <tr>
      <th>非常弱</th>
      <th>数据依赖性的弱</th>
      <th>强制</th>
      <th>顺序一致</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DEC Alpha</td>
      <td>ARM</td>
      <td>X86/64</td>
      <td>dual 386</td>
    </tr>
    <tr>
      <td>C/C++11 low-level atomics</td>
      <td>PowerPC</td>
      <td>SPARC TSO</td>
      <td>Java volatile/C/C++11 atomics</td>
    </tr>
  </tbody>
</table>

<h3 id="section">弱内存模型</h3>

<p>在最弱的内存模型中,可能经历所有四种内存乱序
(<a href="http://g.oswego.edu/dl/jmm/cookbook.html">LoadLoad, StoreStore, LoadStore and StoreLoad</a>).任
何load或store的操作能与任何的其他的load或store操作乱序,只要它不改变一
个独立进程的行为.实际中,这样的乱序由于编译器引起的指令乱序或处理器本身
处理指令的乱序.</p>

<p>当处理器是弱硬件内存模式,通常称它为weakly-ordered或weak ordering.或说
它有relaxed memory model. <strong>DEC Alpha</strong> 是
<a href="http://www.mjmwired.net/kernel/Documentation/memory-barriers.txt#2277">最具代表</a>
的弱排序的处理器.</p>

<p>C/C++的底层原子操作也呈现弱内存模型,无论代码的平台是如x86/64的强序处理
器.下面章节
<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/#memory-ordering-at-compile-time">Memory ordering at compile time</a>
会演示其弱内存模型,并说明如何强制内存顺
序来保护编译器乱序.</p>

<h3 id="section-1">数据依赖性的弱</h3>

<p>ARM和PowerPC系列的处理器内存模型和Alpha同样弱,除了它们保持
<a href="http://www.mjmwired.net/kernel/Documentation/memory-barriers.txt#305">data dependency ordering</a>.它
意味两个相依赖的<code>load</code>(load A, load B&lt;-A)被保证顺序<code>load B&lt;-A</code>总能在
<code>load A</code>之后.(A data dependency barrier is a partial ordering on interdependent loads only; it is not required to have any effect on stores, independent loads or overlapping loads.)</p>

<h3 id="section-2">强内存模型</h3>

<p>弱和强内存模型区别<a href="http://herbsutter.com/2012/08/02/strong-and-weak-hardware-memory-models/#comment-5903">存
在分歧</a>.<a href="http://preshing.com/20120930/weak-vs-strong-memory-models/">Preshing</a>
总结的定义是:</p>

<p><blockquote><p>一个强硬件内存模型是在这样的硬件上每条机器指令隐性的保证acquire and release<br/>semantics的执行.因此,当一个CPU核进行了一串写操作,每个其他的CPU核看到这<br/>些值的改变顺序与其顺序一致.</p></blockquote></p>

<p>所以也就是保证了四种内存乱序
(<a href="http://g.oswego.edu/dl/jmm/cookbook.html">LoadLoad, StoreStore, LoadStore and StoreLoad</a>)
中的3种,除了不保证StoreLoad的顺序.基于以上的定义,x86/64系列处理器基本
就是强顺序的.之后
<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/#memory-ordering-at-processor-time">Memory ordering at processor time</a>
可以看到StoreLoad在X86/64的乱序实验.</p>

<h3 id="section-3">顺序一致</h3>

<p>在顺序一致
(<a href="http://en.wikipedia.org/wiki/Sequential_consistency">Sequential consistency</a>)
的内存模型中,没有内存乱序存在.</p>

<p>如今,很难找到一个现代多核设备保证在硬件层Sequential consistency.也就早
期的386没有强大到能在运行时进行任何内存的乱序.</p>

<p>当用上层语言编程时,Sequential consistency成为一个重要的软件内存模
型.Java5和之后版本,用<code>volatile</code>声明共享变量.在C+11中,可以使用默认的顺
序约束<code>memory_order_seq_cst</code>在做原子操作时.当使用这些术语后,编译器会限
制编译乱序和插入特定CPU的指令来指定合适的memory barrier类型.</p>

<h2 id="memory-ordering-at-compile-time">Memory ordering at compile time</h2>
<p>看如下代码:</p>

<p><code>c test.c
int A, B;
void test() {
  A = B + 1;
  B = 0;
}
</code></p>

<p>不打开编译器的优化,把它编译成汇编,我们可以看到,<code>B</code>的赋值在<code>A</code>的后面,和
原程序的顺序一样.</p>

<p>``` sh
$ gcc -S -masm=intel test.c</p>

<pre><code>mov	eax, DWORD PTR B
add	eax, 1
mov	DWORD PTR A, eax
mov	DWORD PTR B, 0 ```
</code></pre>

<p>用<code>O2</code>打开优化:</p>

<p>``` sh
$ gcc -S -O2  -masm=intel test.c</p>

<pre><code>mov	eax, DWORD PTR B
mov	DWORD PTR B, 0
add	eax, 1
mov	DWORD PTR A, eax ```
</code></pre>

<p>这次编译器把<code>B</code>的赋值提到<code>A</code>的前面.为什么它可以这么做呢?内存顺序的中心
没有破坏.这样的改变并不影响单线程程序,单线程程序不能知道这样的区别.</p>

<p>但是当编写lock-free代码时,这样的编译器乱序就会引起问题.看如下例子,一个
共享的标识来表明其他共享数据是否更新:</p>

<p><code>c
int value;
int updated = 0;
void UpdateValue(int x) {
    value = x;
    update = 1;
}
</code></p>

<p>如果编译器把<code>update</code>的赋值提到<code>value</code>赋值的前面.即使在单核处理器系统中,会
有问题:在两个参数赋值的中间这个线程被中断,使得另外的程序通过<code>update</code>判
断以为<code>value</code>的值已经得到更新,实际上却没有.</p>

<h3 id="compiler-barriers">显性的Compiler Barriers</h3>
<p>一种方法是用一个特殊的被称为Compiler Barrier的指令来防止编译器优化的乱
序.以下
<a href="http://en.wikipedia.org/wiki/Memory_ordering#Compiler_memory_barrier"><code>asm volative</code></a>
是GCC中的方法.</p>

<p><code>c test_barrier.c
int A, B;
void test() {
  A = B + 1;
  asm volatile("" ::: "memory");
  B = 0;
}
</code></p>

<p>经过这样的修改,打开优化,<code>B</code>的存储将保持在要求的顺序上.</p>

<p>``` sh
$ gcc -S -O2  -masm=intel test.c</p>

<pre><code>mov	eax, DWORD PTR B
add	eax, 1
mov	DWORD PTR A, eax
mov	DWORD PTR B, 0 ```
</code></pre>

<h3 id="compiler-barriers-1">隐性的Compiler Barriers</h3>
<p>在C++11中原子库中,每个不是relaxed的原子操作同时是一个compiler barrier.</p>

<p><code>c++
int value;
std::atomic&lt;int&gt; updated(0);
void UpdateValue(int x) {
    value = x;
    // reordering is prevented here
    update.store(1, std::memory_order_release);
}
</code></p>

<p>每一个拥有compiler barrier的函数本身也是一个compiler barrier,即使它是
inline的.</p>

<p><code>c++
int a;
int b;
void DoSomething() {
    a = 1;
    UpdateValue(1);
    b = a + 1;
}
</code></p>

<p>进一步推知,大多数被调用的函数是一个compiler barrier.无论它们是否包含
memory barrier.排除inline函数,被声明为<a href="http://lwn.net/Articles/285332/"><code>pure attribution</code></a>
或当
<a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0474c/CHDHIEGF.html">link-time code generation</a>
使用时.因为编译器在编译时,并不知道<code>UpdateValue</code>的运行是否依赖于<code>a</code>或会
改变<code>a</code>的值从而影响<code>b</code>,所以编译器不会乱序它们之间的顺序.</p>

<p>可以看到,有许多隐藏的规则禁止编译指令的乱序,也防止了编译器多进一步的代
码优化,所以在某些场景
<a href="https://www.kernel.org/doc/Documentation/volatile-considered-harmful.txt">Why the “volatile” type class should not be used</a>,
来让编译器进一步优化.</p>

<h3 id="section-4">无缘由的存储</h3>

<p>有隐形的Compiler Barriers,同样GCC编译器也有无缘由的存储.来自<a href="https://gcc.gnu.org/ml/gcc/2007-10/msg00266.html">这里的实例</a>:</p>

<p>``` c
extern int v;</p>

<pre><code>void
f(int set_v)
{
  if (set_v)
    v = 1;
}
</code></pre>

<p>```</p>

<p>在i686,GCC 3.3.4–4.3.0用<code>O1</code>编译得到:</p>

<p><code>sh
            pushl   %ebp
            movl    %esp, %ebp
            cmpl    $0, 8(%ebp)
            movl    $1, %eax
            cmove   v, %eax        ; load (maybe)
            movl    %eax, v        ; store (always)
            popl    %ebp
            ret
</code></p>

<p>在单线程中,没有问题,但多线程中调用<code>f(0)</code>仅仅只是读取v的值,但中断后回去
覆盖其他线程修改的值.引起
<a href="http://www.devx.com/cplus/Article/42725">data rate</a>.在新的C++11标准中
明确禁止了这样的行为,看<a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3337.pdf">最近C+11标准进行的draft</a>§1.10.22节:</p>

<p><blockquote><p>Compiler transformations that introduce assignments to a potentially shared memory location that would not be modified by the abstract machine are generally precluded by this standard.</p></blockquote></p>

<h2 id="memory-ordering-at-processor-time">Memory ordering at processor time</h2>

<p>看一个简单的CPU乱序的简单例子,即使在强内存模型的X86/64也能看到.有两个
整数<code>X</code>和<code>Y</code>初始是0,另外两个变量r1和r2读取它们的值,两个线程并行运行,执
行如下的机器代码:</p>

<p><img class="center" src="/images/blog/2014/multithreading/ordering-example.png" width="370" height="100" title="‘ordering-example’" ></p>

<p>每个线程存储1到一个共享变量,然后把对方变量读取到一个变量或一个寄存器中.无
论哪个线程先写1到内存,另外个线程读回那个值,意味着最后r1=1或r2=1或两者
都是.但是X86/64是强内存模型,它还是允许<strong>乱序</strong>机器指令.特别,每个线程允许
延迟存储到读回之后.以致最后r1和r2能同时等于0–违反直觉的一个结果.因为
指令可能如下顺序执行:</p>

<p><img class="center" src="/images/blog/2014/multithreading/reordering-example.png" width="190" height="100" title="‘reordering-example’" ></p>

<p>写一个实例程序,实际看一下CPU的确乱序了指令.源码可以
<a href="https://github.com/shishougang/blog_multithreading/tree/master/memory_reordering">Github下载</a>.两
个读写的线程代码如下:</p>

<p>``` c++
sem_t begin_sem1;
sem_t begin_sem2;
sem_t end_sem;</p>

<p>int X, Y;
int r1, r2;</p>

<p>void *ThreadFunc1(void *param) {
  MersenneTwister random(1);
  for (;;) {
    sem_wait(&amp;begin_sem1);
    // random delay
    while (random.Integer() % 8 != 0) {
    }
    X = 1;
    asm volatile(“” ::: “memory”);  // prevent compiler ordering
    r1 = Y;
    sem_post(&amp;end_sem);
  }
  return NULL;
}</p>

<p>void *ThreadFunc2(void *param) {
  MersenneTwister random(2);
  for (;;) {
    sem_wait(&amp;begin_sem2);
    // random delay
    while (random.Integer() % 8 != 0) {
    }
    Y = 1;
    asm volatile(“” ::: “memory”);  // prevent compiler ordering
    r2 = X;
    sem_post(&amp;end_sem);
  }
  return NULL;
}
```</p>

<p>随机的延迟被插入在存储的开始处,为了交错线程的开始时间,以来达到重叠两个线程
的指令的目的.随机延迟使用线程安全的<code>MersenneTwister</code>类.汇编代码<code>asm
volatile("" ::: "memory");</code>如上节所述只是用来
<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/#memory-ordering-at-compile-time">防止编译器的乱序</a>,
因为这里是要看CPU的乱序,排除编译器的乱序影响.</p>

<p>主线程如下,利用
<a href="http://pubs.opengroup.org/onlinepubs/7908799/xsh/sem_init.html">POSIX的semaphore</a>
同步它与两个子线程的同步.先让两个子线程等待,直到主线程初始化<code>X=0</code>和
<code>Y=0</code>.然后主线程等待,直到两个子线程完成操作,然后主线程检查<code>r1</code>和<code>r2</code>的
值.所以semaphore防止线程见的不同步引起的内存乱序,主线程代码如下:</p>

<p>``` c++
int main(int argc, char *argv[]) {
  sem_init(&amp;begin_sem1, 0, 0);
  sem_init(&amp;begin_sem2, 0, 0);
  sem_init(&amp;end_sem, 0, 0);</p>

<p>pthread_t thread[2];
  pthread_create(&amp;thread[0], NULL, ThreadFunc1, NULL);
  pthread_create(&amp;thread[1], NULL, ThreadFunc2, NULL);</p>

<p>int detected = 0;
  for (int i = 1; ; ++i) {
    X = 0;
    Y = 0;
    sem_post(&amp;begin_sem1);
    sem_post(&amp;begin_sem2);
    sem_wait(&amp;end_sem);
    sem_wait(&amp;end_sem);
    if (r1 == 0 &amp;&amp; r2 == 0) {
      detected++;
      printf(“%d reorders detected after %d iterations\n”, detected, i);
    }
  }
  return 0;
}
```</p>

<p>在Intel i5-2435M X64的ubuntu下运行一下程序:</p>

<p><code>sh
1 reorders detected after 2181 iterations
2 reorders detected after 4575 iterations
3 reorders detected after 7689 iterations
4 reorders detected after 22215 iterations
5 reorders detected after 60023 iterations
6 reorders detected after 60499 iterations
7 reorders detected after 61639 iterations
8 reorders detected after 62243 iterations
9 reorders detected after 67998 iterations
10 reorders detected after 68098 iterations
11 reorders detected after 71179 iterations
12 reorders detected after 71668 iterations
13 reorders detected after 72417 iterations
14 reorders detected after 73970 iterations
15 reorders detected after 78227 iterations
16 reorders detected after 81897 iterations
17 reorders detected after 82722 iterations
18 reorders detected after 85377 iterations
...
</code></p>

<p>差不多每 <strong>4000</strong> 次的迭代才发现一次CPU内存乱序.所以多线程的bug是多么难
发现.那么如何消除这些乱序.至少有如下两种方法:</p>

<ol>
  <li>让两个子线程在同一个CPU核下运行.(没有可移植性方法,如下是linux平台的).</li>
  <li>使用CPU的memory barrier防止它的乱序.</li>
</ol>

<h3 id="lock-to-one-processor">Lock to one processor</h3>
<p>让两个子线程在同一个CPU核下运行,代码如下:</p>

<p><code>c++
  cpu_set_t cpus;
  CPU_ZERO(&amp;cpus);
  CPU_SET(0, &amp;cpus);
  pthread_setaffinity_np(thread[0], sizeof(cpu_set_t), &amp;cpus);
  pthread_setaffinity_np(thread[1], sizeof(cpu_set_t), &amp;cpus);
</code></p>

<h3 id="place-a-memory-barrier">Place a memory barrier</h3>

<p>防止一个Store在Load之后的乱序,需要一个StoreLoad的barrier.这里使用
<code>mfence</code>的一个全部memory barrier,防止任何类型的内存乱序.代码如下:</p>

<p><code>c++
void *ThreadFunc1(void *param) {
  MersenneTwister random(1);
  for (;;) {
    sem_wait(&amp;begin_sem1);
    // random delay
    while (random.Integer() % 8 != 0) {
    }
    X = 1;
    asm volatile("mfence" ::: "memory");  // prevent CPU ordering
    r1 = Y;
    sem_post(&amp;end_sem);
  }
  return NULL;
  }
</code></p>

<h2 id="more">More</h2>

<ol>
  <li><a href="http://www.cl.cam.ac.uk/~pes20/weakmemory/">University of Cambridge整理的文档和论文</a></li>
  <li><a href="http://lwn.net/Articles/470681/">Paul McKenney概括他们做的一些工作和工具</a></li>
  <li><a href="http://www.amazon.com/gp/product/0123973376/ref=as_li_ss_tl?ie=UTF8&amp;tag=preshonprogr-20&amp;linkCode=as2&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0123973376">The Art of Multiprocessor Programming</a></li>
  <li><a href="http://www.amazon.com/C-Concurrency-Action-Practical-Multithreading/dp/1933988770/ref=pd_sim_b_2?ie=UTF8&amp;refRID=1QTX99XZAM6HKVG7X0G2">C++ Concurrency in Action: Practical Multithreading</a></li>
  <li><a href="https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.2011.01.02a.pdf">Is Parallel Programming Hard, And, If So, What Can You Do About It?</a></li>
</ol>

<h2 id="summarization">Summarization</h2>
<ol>
  <li>有两种内存乱序存在:编译器乱序和CPU乱序.</li>
  <li>如何防止编译器乱序.</li>
  <li>如何防止CPU乱序.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Double-Checked Locking Works In C++11]]></title>
    <link href="http://dreamrunner.org/blog/2014/06/22/double-checked-locking-works-in-c-plus-plus-11/"/>
    <updated>2014-06-22T14:07:01+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/06/22/double-checked-locking-works-in-c-plus-plus-11</id>
    <content type="html"><![CDATA[<p>在
<a href="http://dreamrunner.org/blog/2014/05/03/%E6%B5%85%E8%B0%88%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F6/">浅谈设计模式六: 单例模式(Singleton)</a>
中提到double-checked locking pattern(DCLP)来实现Singleton设计模式，但是
在C++11之前，没有安全方法在可移植的C++中去实现它．具体原因可见
<a href="http://dreamrunner.org/blog/2014/05/03/%E6%B5%85%E8%B0%88%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F6/">单例模式(Singleton)</a>
或Scott Meyers和Andrei Alexandrescu发布的原文
<a href="http://www.aristeia.com/Papers/DDJ_Jul_Aug_2004_revised.pdf">“C++ and the Perils of Double-Checked Locking”</a>
．</p>

<p>C++11引入了新的内存模型和线程库，使得能在C++中实现可移植的DCLP．本文说
明如何实现它．</p>

<!-- more -->

<h2 id="double-checked-locking">什么是Double-Checked Locking</h2>
<p>在
<a href="http://dreamrunner.org/blog/2014/05/03/%E6%B5%85%E8%B0%88%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F6/">单例模式(Singleton)</a>
很好的介绍什么是DCLP,这里稍作回顾.</p>

<p>线程安全的方式实现Signleton模式如下:</p>

<p><code>c++ singleton.cc
Singleton* Singleton::instance() {
  Lock lock;    // acquire lock (params omitted for simplicity)
  if(pInstance == NULL) {
    pInstance = new Singleton();
  }
  return pInstance;
  }  // release lock (via Lock destructor)
</code></p>

<p>每次获取 Singleton 都要获取一个锁，但是实际上，我们只有当初始化 pInstance 时才需要一个锁。也就是只发生在第一次调用 instance 时。如果在一个程序运行时， instance 被调用了n次，我们只需要锁在第一次调用时。当我们知道那n-1次锁是没必要的.</p>

<p>DCLP的关键点是发现，大多数 instance 的调用将看到 pInstance 是非空的，因此根本没必要去尝试初始化它。因此，DCLP判断 pInstance 是否为空在尝试获取锁前。只有当判断成功（ pInstance 还没有被初始化）才去获取锁，然后之后这个判断在此进行一次确保 pInstance 是仍然空的。（所以名字叫双重检查锁）。第二个检查是有必要的，因为从上可以看到，另外的线程可能碰巧初始化了 pInstance 在 pInstance 被第一次判断和获取锁之间。</p>

<p><code>c++ singleton-dclp.cc
Singleton* Singleton::instance() {
  Singleton *tmp = pInstance;
  ...  // need memory barrier
  if(tmp == 0) { // 1st test
  Lock lock;
  tmp = pInstance;
  if(tmp == 0) { // 2nd test
    tmp  = new Singleton;
  ...  // need memory barrier
    pInstance = tmp;
  }
  }
return pInstance;
}
</code></p>

<p><a href="http://dreamrunner.org/blog/2014/05/03/%E6%B5%85%E8%B0%88%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F6/">单例模式(Singleton)</a>
说明了各种不安全实现的缺陷,主要原因是1) 编译器的乱序编译 和2) CPU的乱
序执行指令.所以安全的实现依靠memory barrier,防止它们的乱序,使得在多线
程中得到同步,C++11之前没有可移植的C/C++函数,但现在,C++11有了.</p>

<h2 id="c11acqurerelease-fence">使用C++11的Acqure和Release Fence</h2>
<p>使用Acqure和Release Fence来实现它,并且保证对实例<code>pInstance</code>进行原子操
作,把它定义为<code>atomic</code>类型,并用<code>memory_order_relaxed</code>操作.(Relaxed
ordering: there are no synchronization or ordering constraints, only
atomicity is required of this operation.)如下实现代码.</p>

<p>``` c++
std::atomic&lt;Singleton *&gt; Singleton::m_pInstance;
std::mutex Singleton::m_mutex;</p>

<p>Singleton* Singleton::instance() {
  Singleton *tmp = m_pInstance.load(std::memory_order_relaxed);
  std::atomic_thread_fence(std::memory_order_acquire);
  if(tmp == nullptr) {
    std::lock_guard<std::mutex> lock(m_mutex);
    tmp = m_pInstance.load(std::memory_order_relaxed);
    if(tmp == nullptr) {
      tmp  = new Singleton;
      std::atomic_thread_fence(std::memory_order_release);
      m_pInstance.store(tmp, std::memory_order_relaxed);
    }
  }
  return m_pInstance;
}
```</std::mutex></p>

<p>在多核系统中,这整个代码也是稳健的,因为memory fences在多个线程间建立了
同步的关系.<code>Singleton::m_pInstance</code>作为guard variable,singleton变
量自身成为payload.</p>

<p>如果没有这层同步关系的话,就不能保证第一个线程的所有写操作(这里就是
singleton实力的创建)被第二个线程读取到,即使<code>m_pInstance</code>已经被第二个线
程能看到.</p>

<h2 id="c11">使用C++11的底层的内存顺序约束</h2>
<p>在C++11中也可以在单元操作时附加底层的内存顺序约束来达到同样的目的.一个
write-release能同步于一个read-release.</p>

<ol>
  <li>
    <p><code>memory_order_acquire</code>: A load operation with this memory order performs the acquire operation on the affected memory location: prior writes made to other memory locations by the thread that did the release become visible in this thread.</p>
  </li>
  <li>
    <p><code>memory_order_release</code>: A store operation with this memory order performs the release operation: prior writes to other memory locations become visible to the threads that do a consume or an acquire on the same location.</p>
  </li>
</ol>

<p>``` c++
std::atomic&lt;Singleton *&gt; Singleton::m_pInstance;
std::mutex Singleton::m_mutex;</p>

<p>Singleton* Singleton::instance() {
  Singleton *tmp = m_pInstance.load(std::memory_order_acquire);
  if(tmp == nullptr) {
    std::lock_guard<std::mutex> lock(m_mutex);
    tmp = m_pInstance.load(std::memory_order_relaxed);
    if(tmp == nullptr) {
      tmp  = new Singleton;
      m_pInstance.store(tmp, std::memory_order_release);
    }
  }
  return m_pInstance;
}
```</std::mutex></p>

<p>从深层分析来看,这种形式的免锁机制的同步比上面单独memory fences来的约束
更小.这种形式的操作只意味在这个操作周围防止内存乱序,而memory fences意
味着在一块区域内防止内存乱序.更多细节参考preshing的
<a href="http://preshing.com/20131125/acquire-and-release-fences-dont-work-the-way-youd-expect/">Acquire and Release Fences Don’t Work the Way You’d Expect</a>
的分析.
## 使用C++11的Sequentially-consistent ordering
C++11还提供了其他的方法来写lock-free的代码.当在atomic操作函数中忽略
<code>std::memory_order</code>参数项,那么默认值是<code>std::memory_order_seq_cst</code>,使得
所有原子参数成为
<a href="http://en.wikipedia.org/wiki/Sequential_consistency">sequentically consistent(SC)</a>
原子.通过SC原子性,整个算法保证sequentically consistent只要没有<a href="http://www.devx.com/cplus/Article/42725">data races</a>.</p>

<p>``` c++
std::atomic&lt;Singleton *&gt; Singleton::m_pInstance;
std::mutex Singleton::m_mutex;</p>

<p>Singleton* Singleton::instance() {
  Singleton *tmp = m_pInstance.load();
  std::atomic_thread_fence(std::memory_order_acquire);
  if(tmp == nullptr) {
    std::lock_guard<std::mutex> lock(m_mutex);
    tmp = m_pInstance.load(std::memory_order_relaxed);
    if(tmp == nullptr) {
      tmp  = new Singleton;
      std::atomic_thread_fence(std::memory_order_release);
      m_pInstance.store(tmp);
    }
  }
  return m_pInstance;
}
```</std::mutex></p>

<p>SC的原子性可能更容易理解.权衡点就是它产生的机器代码没有之前做法的高效.比
如如下是Gcc 4.8.2 intle X64对上面代码产生的机器代码,通过<code>g++ -O2 -std=c++11 -S</code>.
<img src="/images/blog/2014/multithreading/sc.png" title="sc’" ></p>

<p>因为使用了SC原子性,对<code>m_pInstance</code>的存储实现使用了<code>mfence</code>指令,起到一
个在X64上的full memory fence.这是个更严格的指令想对于DCLP在X64上的实际
需求.一个普通的<code>mov</code>足以胜任.但也无关紧要,因为<code>mfence</code>指令也仅仅执行一
次而已,就在创建singleton的实例的代码路径上.</p>

<h2 id="more">More</h2>
<p>使用<a href="http://preshing.com">Preshing</a>的小型可移植的lock-free库,在没有C++11
的支持下,使用它的<a href="http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/#using-mintomic-fences">Mintomic Fences实现DCLP</a>.</p>

<p>更多关于C++11的multithreading库的详解见之后的文章.</p>
]]></content>
  </entry>
  
</feed>
