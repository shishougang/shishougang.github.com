<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: C++ | (learn&think)]]></title>
  <link href="http://dreamrunner.org/tags/c-/atom.xml" rel="self"/>
  <link href="http://dreamrunner.org/"/>
  <updated>2014-07-03T20:06:35+08:00</updated>
  <id>http://dreamrunner.org/</id>
  <author>
    <name><![CDATA[DreamRunner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[浅谈Mutex (Lock)]]></title>
    <link href="http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock+%28shougang-MacBookPro%27s+conflicted+copy+2014-07-02%29/"/>
    <updated>2014-06-29T20:52:09+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock (shougang-MacBookPro's conflicted copy 2014-07-02)</id>
    <content type="html"><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Mutual_exclusion">Mutex</a>(又叫Lock),在多线程中,作为同步的基本类型,用来保证没有两个线程或进程同时在他们的关键区域.因为Mutex这种排它性,很多人认为Mutex开销很大,尽量避免使用它.就如这篇
分析完共享数据问题后,进一步分析说明
<a href="http://courses.cs.washington.edu/courses/cse451/03wi/section/prodcons.htm">Avoiding locks</a>
来解决这个问题.但Mutex真的开销如此大,还是被大家误解了?Matthew
Dillon<a href="http://groups.google.com/group/net.micro.mac/msg/752d18de371bd65c?dmode=source">写道</a>,”Most
people have the misconception that locks are slow.”, Jeff Preshing也
<a href="http://preshing.com/20111118/locks-arent-slow-lock-contention-is/">写了这篇”Locks Aren’t Slow; Lock Contention Is”</a>.</p>

<p>那么接下来做3个关于Mutex的Benchmark,具体分析一下Mutex的开销如何,最后并
利用原子操作和semaphore实现一个lightweight Mutex.</p>

<!-- more -->

<p>一个Mutex仅仅从Lock到Unlock具体开销是多少,是不是占用很多时间,从
<a href="http://preshing.com/20111124/always-use-a-lightweight-mutex/">Always Use a Lightweight Mutex</a>
从可以看到在windows中有两种
Mutex:<a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms684266%28v=vs.85%29.aspx">Muetx</a>
和
<a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms682530%28v=vs.85%29.aspx">Critical Section</a>,
重量级和轻量级的区别,两者的时间开销相差25倍多,所以一直使用轻量级的Mutex.</p>

<p><a href="http://ridiculousfish.com/blog/posts/barrier.html">这篇文章</a>在高强度
下lock的性能:每个线程做任何事情都占用lock(高冲突),lock占用极短的时间
(高频率).值得一读,但是在实际应用中,基本避免如此使用locks.这里对
Mutex Contention和Mutex Frequency都做最好和最坏场景的使用测试.</p>

<p>Mutex被灌以避免使用也因为其他原因.现在有很多大家熟知的
<a href="en.wikipedia.org/wiki/Non-blocking_algorithma">lock-free programming</a>
技术.Lock-free编程非常具有挑战性,但在实际场景中获得巨大的性能.既然有
lock-free的技术吸引我们使用它们,那么locks就显得索然无味了.</p>

<p>但也不能因此忽略lock.因为在实际很多场景,它仍然是利器.</p>

<h2 id="lightweight-mutex-benchmark">Lightweight Mutex Benchmark</h2>

<p>Linux下的POSIX thread是轻量级的Mutex.基于Linux特有的
<a href="http://en.wikipedia.org/wiki/Futex">futex</a>技术,当没有其他线程竞争锁时它被优化过.使
用如下简单的例子,测试一个单线程lock和unlock,所有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/mutex_time">Github上</a>.</p>

<p><code>c++
pthread_mutex_init(&amp;lock, NULL);
const int kN = 1000000;
for (int i = 0; i &lt; kN; ++i) {
    pthread_mutex_lock(&amp;lock);
    pthread_mutex_unlock(&amp;lock);
}
pthread_mutex_destroy(&amp;lock);
</code></p>

<p>插入相应的时间代码,算出10万次的单线程lock/unlock平均时间.在不同的处理
器下,结果如下:</p>

<p><img class="center" src="/images/blog/2014/multithreading/mutex_benchmark.png" width="450" height="200" title="‘mutex_benchmark’" ></p>

<p>如果假设一个线程每分钟获取1e5次mutex,并且没有其他线程与它竞争.基于如下
的图,可预计0.2%到0.4%的开销.不算差.在比较低频率下,开销基本忽略不计.之
后<a href="http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock#build-own-lightweight-mutex">Build own lightweight mutex</a>,会利用<a href="http://en.wikipedia.org/wiki/Semaphore_%28programming%29">semaphore</a>和一个原子操作,实现一个lightweight mutex.</p>

<p>POSIX thread与Windows Critical Section不同,它不仅支持线程间的同步,
还支持进程间的同步.实例代码如下:</p>

<p>```  c++  mutex_between_process.cc
pthread_mutex_t mutex;
pthread_mutexattr_t attrmutex;</p>

<p>/* Initialise attribute to mutex. */
pthread_mutexattr_init(&amp;attrmutex);
pthread_mutexattr_setpshared(&amp;attrmutex, PTHREAD_PROCESS_SHARED);
pthread_mutex_init(&amp;mutex, &amp;attrmutex);</p>

<p>/* Use the mutex. */</p>

<p>/* Clean up. */
pthread_mutex_destroy(pmutex);
pthread_mutexattr_destroy(&amp;attrmutex);
```</p>

<h2 id="mutex-contention-benchmark">Mutex Contention Benchmark</h2>

<p>在测试中,产生一个不断生成随机数的线程,使用自己编制的线程安全的
<a href="http://en.wikipedia.org/wiki/Mersenne_twister">Mersenne Twister</a>实现
代码.每过一段时间,它获取和释放一个锁,获取和释放锁之间的时间每次是随机的,但
是总的平均时间是提前设计好的.这个随机的过程就是个泊松分布过程,计算出产
生一个随机数的平均时间—ns在2.93 GHz i7上,把它作为运行单位.利用
<a href="http://wiki.dreamrunner.org/public_html/Algorithms/Theory%20of%20Algorithms/poisson-process.html">Poisson Process</a>
的算法决定运行多少个运行单位在获取和释放锁之间.并利用
<a href="http://dreamrunner.org/blog/2014/06/24/high-resolution-time/">High Resolution Time</a>API
计算时间.这个线程的代码如下,所有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/mutex_contention">Github上</a>:</p>

<p>``` c++
  GetMonotonicTime(&amp;start);
  for (;;) {
    work_units = static_cast<int> (random.PoissonInterval(
        global_state.average_unlock_count) + 0.5f);
    for (int i = 0; i &lt; work_units; ++i) {
      random.Integer();
    }
    thread_stats.workdone += work_units;</int></p>

<pre><code>GetMonotonicTime(&amp;end);
elapsed_time = GetElapsedTime(&amp;start, &amp;end);
if (elapsed_time &gt;= global_state.time_limit) {
  break;
}

// Do some work while holding the lock
pthread_mutex_lock(&amp;global_state.thread_mutex);
work_units = static_cast&lt;int&gt; (random.PoissonInterval(
    global_state.average_locked_count) + 0.5f);
for (int i = 0; i &lt; work_units; ++i) {
  random.Integer();
}
thread_stats.workdone += work_units;
pthread_mutex_unlock(&amp;global_state.thread_mutex);

thread_stats.iterations++;
GetMonotonicTime(&amp;end);
elapsed_time = GetElapsedTime(&amp;start, &amp;end);
if (elapsed_time &gt;= global_state.time_limit) {
  break;
}   } ```
</code></pre>

<p>这里模拟获取和释放15000次锁每秒,从1个线程运行到2个线程,最后到4个线
程.并且验证占用锁的时间,从0%到100%的每次运行时间占用锁.把1个线程的完成
的工作量作为基准数据,其他的去除以它,计算相对增益.基本测试方案如下:</p>

<p><code>c++
// Test 15000 locks per second: thread number, lock_interval
    1, 1/15000.0f, 
    2, 1/15000.0f,
    3, 1/15000.0f,
    4, 1/15000.0f,
</code></p>

<p>从图中看出,随着锁占用的时间增加,并行性越来越差,直到最后占用90%以后,单
线程运行的更好.可以说,短时间的占用锁的时间,以10%以内,系统达到很高的并
行性.虽然并不是完美的,但是也接近.锁总体很快.</p>

<p>把这个结果放到实际中,Jeff Preshing在
<a href="http://preshing.com/20111118/locks-arent-slow-lock-contention-is/">这篇</a>
提到,实际的游戏程序中,15000的锁每秒来自3个线程,占用锁的时间相对2%.在图
中很适中的区域.</p>

<h2 id="mutex-frequency-benchmark">Mutex Frequency Benchmark</h2>

<p>尽管一个lightweight mutex有开销,但如上测试在2.40GHz i5上,lock/unlock锁
开销约 <strong>34.2ns</strong> ,因此15000锁每秒开销很低以致不是严重影响结果.那么把
锁的每秒频率提高呢?</p>

<p>只创建2个线程,进行一系列的锁的每秒频率测试在2.40GHz i5上,从占用锁时间
10 ns(1e8/s)到100 us(1e4/s),用单线程的占用锁时间10 ms作为基准工作量,其
他与它比较,测试方案如下:</p>

<p>``` c++
  // Reference
  1, 10e-3f,      // 10 ms        100/s</p>

<pre><code>// Test various lock rates with 2 threads
2, 10e-9f,      // 10 ns        100000000/s
2, 31.6e-9f,    // 31.6 ns      31600000/s
2, 100e-9f,     // 100 ns       10000000/s
2, 316e-9f,     // 316 ns       3160000/s
2, 1e-6f,       // 1 us         1000000/s
2, 3.16e-6f,    // 3.16 us      316000/s
2, 10e-6f,      // 10 us        100000/s
2, 31.6e-6f,    // 31.6 us      31600/s
2, 100e-6f,     // 100 us       10000/s ```
</code></pre>

<p><img src="/images/blog/2014/multithreading/frequency_benchmark.png" title="‘frequency_bechmark’" ></p>

<p>如预想一样,对于非常高频率的锁,锁的开销开始减少实际工作量.在网络上,可以
找到很多同样的测试.图中下边的线条,对于这样高的频率,也就是占用锁的时间
很短,就一些CPU的指令,这样的情况下,当锁之间的工作如此简单,那么一个
lock-free的实现更适合.</p>

<h2 id="build-own-lightweight-mutex">Build own lightweight mutex</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[浅谈Mutex (Lock)]]></title>
    <link href="http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock/"/>
    <updated>2014-06-29T20:52:09+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock</id>
    <content type="html"><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Mutual_exclusion">Mutex</a>(又叫Lock),在多线程中,作为同步的基本类型,用来保证没有两个线程或进程同时在他们的关键区域.因为Mutex这种排它性,很多人认为Mutex开销很大,尽量避免使用它.就如这篇
分析完共享数据问题后,进一步分析说明
<a href="http://courses.cs.washington.edu/courses/cse451/03wi/section/prodcons.htm">Avoiding locks</a>
来解决这个问题.但Mutex真的开销如此大,还是被大家误解了?Matthew
Dillon<a href="http://groups.google.com/group/net.micro.mac/msg/752d18de371bd65c?dmode=source">写道</a>,”Most
people have the misconception that locks are slow.”, Jeff Preshing也
<a href="http://preshing.com/20111118/locks-arent-slow-lock-contention-is/">写了这篇”Locks Aren’t Slow; Lock Contention Is”</a>.</p>

<p>那么接下来做3个关于Mutex的Benchmark,具体分析一下Mutex的开销如何,最后并
利用原子操作和semaphore实现一个lightweight Mutex.</p>

<!-- more -->

<p>一个Mutex仅仅从Lock到Unlock具体开销是多少,是不是占用很多时间,从
<a href="http://preshing.com/20111124/always-use-a-lightweight-mutex/">Always Use a Lightweight Mutex</a>
从可以看到在windows中有两种
Mutex:<a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms684266%28v=vs.85%29.aspx">Muetx</a>
和
<a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms682530%28v=vs.85%29.aspx">Critical Section</a>,
重量级和轻量级的区别,两者的时间开销相差25倍多,所以一直使用轻量级的Mutex.</p>

<p><a href="http://ridiculousfish.com/blog/posts/barrier.html">这篇文章</a>在高强度
下lock的性能:每个线程做任何事情都占用lock(高冲突),lock占用极短的时间
(高频率).值得一读,但是在实际应用中,基本避免如此使用locks.这里对
Mutex Contention和Mutex Frequency都做最好和最坏场景的使用测试.</p>

<p>Mutex被灌以避免使用也因为其他原因.现在有很多大家熟知的
<a href="en.wikipedia.org/wiki/Non-blocking_algorithma">lock-free programming</a>
技术.Lock-free编程非常具有挑战性,但在实际场景中获得巨大的性能.既然有
lock-free的技术吸引我们使用它们,那么locks就显得索然无味了.</p>

<p>但也不能因此忽略lock.因为在实际很多场景,它仍然是利器.</p>

<h2 id="lightweight-mutex-benchmark">Lightweight Mutex Benchmark</h2>

<p>Linux下的POSIX thread是轻量级的Mutex.基于Linux特有的
<a href="http://en.wikipedia.org/wiki/Futex">futex</a>技术,当没有其他线程竞争锁时它被优化过.使
用如下简单的例子,测试一个单线程lock和unlock,所有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/mutex_time">Github上</a>.</p>

<p><code>c++
pthread_mutex_init(&amp;lock, NULL);
const int kN = 1000000;
for (int i = 0; i &lt; kN; ++i) {
    pthread_mutex_lock(&amp;lock);
    pthread_mutex_unlock(&amp;lock);
}
pthread_mutex_destroy(&amp;lock);
</code></p>

<p>插入相应的时间代码,算出10万次的单线程lock/unlock平均时间.在不同的处理
器下,结果如下:</p>

<p><img class="center" src="/images/blog/2014/multithreading/mutex_benchmark.png" width="450" height="200" title="‘mutex_benchmark’" ></p>

<p>如果假设一个线程每分钟获取1e5次mutex,并且没有其他线程与它竞争.基于如下
的图,可预计0.2%到0.4%的开销.不算差.在比较低频率下,开销基本忽略不计.之
后<a href="http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock#build-own-lightweight-mutex">Build own lightweight mutex</a>,会利用<a href="http://en.wikipedia.org/wiki/Semaphore_%28programming%29">semaphore</a>和一个原子操作,实现一个lightweight mutex.</p>

<p>POSIX thread与Windows Critical Section不同,它不仅支持线程间的同步,
还支持进程间的同步.实例代码如下:</p>

<p>```  c++  mutex_between_process.cc
pthread_mutex_t mutex;
pthread_mutexattr_t attrmutex;</p>

<p>/* Initialise attribute to mutex. */
pthread_mutexattr_init(&amp;attrmutex);
pthread_mutexattr_setpshared(&amp;attrmutex, PTHREAD_PROCESS_SHARED);
pthread_mutex_init(&amp;mutex, &amp;attrmutex);</p>

<p>/* Use the mutex. */</p>

<p>/* Clean up. */
pthread_mutex_destroy(pmutex);
pthread_mutexattr_destroy(&amp;attrmutex);
```</p>

<h2 id="mutex-contention-benchmark">Mutex Contention Benchmark</h2>

<p>在测试中,产生一个不断生成随机数的线程,使用自己编制的线程安全的
<a href="http://en.wikipedia.org/wiki/Mersenne_twister">Mersenne Twister</a>实现
代码.每过一段时间,它获取和释放一个锁,获取和释放锁之间的时间每次是随机的,但
是总的平均时间是提前设计好的.这个随机的过程就是个泊松分布过程,计算出产
生一个随机数的平均时间—ns在2.93 GHz i7上,把它作为运行单位.利用
<a href="http://wiki.dreamrunner.org/public_html/Algorithms/Theory%20of%20Algorithms/poisson-process.html">Poisson Process</a>
的算法决定运行多少个运行单位在获取和释放锁之间.并利用
<a href="http://dreamrunner.org/blog/2014/06/24/high-resolution-time/">High Resolution Time</a>API
计算时间.这个线程的代码如下,所有代码在<a href="https://github.com/shishougang/blog_multithreading/tree/master/mutex_contention">Github上</a>:</p>

<p>``` c++
  GetMonotonicTime(&amp;start);
  for (;;) {
    work_units = static_cast<int> (random.PoissonInterval(
        global_state.average_unlock_count) + 0.5f);
    for (int i = 0; i &lt; work_units; ++i) {
      random.Integer();
    }
    thread_stats.workdone += work_units;</int></p>

<pre><code>GetMonotonicTime(&amp;end);
elapsed_time = GetElapsedTime(&amp;start, &amp;end);
if (elapsed_time &gt;= global_state.time_limit) {
  break;
}

// Do some work while holding the lock
pthread_mutex_lock(&amp;global_state.thread_mutex);
work_units = static_cast&lt;int&gt; (random.PoissonInterval(
    global_state.average_locked_count) + 0.5f);
for (int i = 0; i &lt; work_units; ++i) {
  random.Integer();
}
thread_stats.workdone += work_units;
pthread_mutex_unlock(&amp;global_state.thread_mutex);

thread_stats.iterations++;
GetMonotonicTime(&amp;end);
elapsed_time = GetElapsedTime(&amp;start, &amp;end);
if (elapsed_time &gt;= global_state.time_limit) {
  break;
}   } ```
</code></pre>

<p>这里模拟获取和释放15000次锁每秒,从1个线程运行到2个线程,最后到4个线
程.并且验证占用锁的时间,从0%到100%的每次运行时间占用锁.把1个线程的完成
的工作量作为基准数据,其他的去除以它,计算相对增益.基本测试方案如下:</p>

<p><code>c++
// Test 15000 locks per second: thread number, lock_interval
    1, 1/15000.0f, 
    2, 1/15000.0f,
    3, 1/15000.0f,
    4, 1/15000.0f,
</code></p>

<p>从图中看出,随着锁占用的时间增加,并行性越来越差,直到最后占用90%以后,单
线程运行的更好.可以说,短时间的占用锁的时间,以10%以内,系统达到很高的并
行性.虽然并不是完美的,但是也接近.锁总体很快.</p>

<p>把这个结果放到实际中,Jeff Preshing在
<a href="http://preshing.com/20111118/locks-arent-slow-lock-contention-is/">这篇</a>
提到,实际的游戏程序中,15000的锁每秒来自3个线程,占用锁的时间相对2%.在图
中很适中的区域.</p>

<h2 id="mutex-frequency-benchmark">Mutex Frequency Benchmark</h2>

<p>尽管一个lightweight mutex有开销,但如上测试在2.40GHz i5上,lock/unlock锁
开销约 <strong>34.2ns</strong> ,因此15000锁每秒开销很低以致不是严重影响结果.那么把
锁的每秒频率提高呢?</p>

<p>只创建2个线程,进行一系列的锁的每秒频率测试在2.40GHz i5上,从占用锁时间
10 ns(1e8/s)到100 us(1e4/s),用单线程的占用锁时间10 ms作为基准工作量,其
他与它比较,测试方案如下:</p>

<p>``` c++
  // Reference
  1, 10e-3f,      // 10 ms        100/s</p>

<pre><code>// Test various lock rates with 2 threads
2, 10e-9f,      // 10 ns        100000000/s
2, 31.6e-9f,    // 31.6 ns      31600000/s
2, 100e-9f,     // 100 ns       10000000/s
2, 316e-9f,     // 316 ns       3160000/s
2, 1e-6f,       // 1 us         1000000/s
2, 3.16e-6f,    // 3.16 us      316000/s
2, 10e-6f,      // 10 us        100000/s
2, 31.6e-6f,    // 31.6 us      31600/s
2, 100e-6f,     // 100 us       10000/s ```
</code></pre>

<p><img src="/images/blog/2014/multithreading/frequency_benchmark.png" title="‘frequency_bechmark’" ></p>

<p>如预想一样,对于非常高频率的锁,锁的开销开始减少实际工作量.在网络上,可以
找到很多同样的测试.图中下边的线条,对于这样高的频率,也就是占用锁的时间
很短,就一些CPU的指令,这样的情况下,当锁之间的工作如此简单,那么一个
lock-free的实现更适合.</p>

<p>我们获得了一大块关于锁的性能:从它进行很好的情况,到缓慢应用的情况.在考
虑实际锁的使用情况,不能说所有锁都是慢的.必须承认,很容易乱用锁,但不用太
担心,任何的瓶颈问题都会在细心的profiling中发现.当你考虑锁是如何的稳定,
相对容易的理解它们(与lock-free技术相比),锁有时候其实很好用.</p>

<h2 id="build-own-lightweight-mutex">Build own lightweight mutex</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[浅谈Memory Reordering]]></title>
    <link href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/"/>
    <updated>2014-06-28T22:55:22+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering</id>
    <content type="html"><![CDATA[<h2 id="memory-ordering">Memory ordering</h2>
<p>在我们编写的C/C++代码和它被在CPU上运行,按照一些规则,代码的内存交互会被
乱序.内存乱序同时由编译器(编译时候)和处理器(运行时)造成,都为了使代码运
行的更快.</p>

<p><img src="/images/blog/2014/multithreading/memory_model.png" title="‘memory_ordering’" ></p>

<p>被编译开发者和处理器制造商遵循的中心内存排序准则是:
<blockquote><p>不能改变单线程程序的行为.</p></blockquote></p>

<p>因为这条规则,在写单线程代码时内存乱序被普遍忽略.即使在多线程程序中,它
也被时常忽略,因为有mutexes,semaphores等来防止它们调用中的内存乱序.仅当
lock-free技术被使用时,内存在不受任何互斥保护下被多个线程共享,内存乱序
的影响能被看到.</p>

<p>下面先比较Weak和Strong的内存模型,然后分两部分,实际内存乱序如何在编译和运行时发生,并如何防止它们.</p>

<!-- more -->

<h2 id="weak-vs-strong-memory-models">Weak VS strong Memory Models</h2>
<p><a href="http://preshing.com/about">Jeff Preshing</a>在
<a href="http://preshing.com/20120930/weak-vs-strong-memory-models/">Weak vs. Strong Memory Models</a>
中很好的总结了从Weak到Strong的类型:</p>

<table>
  <thead>
    <tr>
      <th>非常弱</th>
      <th>数据依赖性的弱</th>
      <th>强制</th>
      <th>顺序一致</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DEC Alpha</td>
      <td>ARM</td>
      <td>X86/64</td>
      <td>dual 386</td>
    </tr>
    <tr>
      <td>C/C++11 low-level atomics</td>
      <td>PowerPC</td>
      <td>SPARC TSO</td>
      <td>Java volatile/C/C++11 atomics</td>
    </tr>
  </tbody>
</table>

<h3 id="section">弱内存模型</h3>

<p>在最弱的内存模型中,可能经历所有四种内存乱序
(<a href="http://g.oswego.edu/dl/jmm/cookbook.html">LoadLoad, StoreStore, LoadStore and StoreLoad</a>).任
何load或store的操作能与任何的其他的load或store操作乱序,只要它不改变一
个独立进程的行为.实际中,这样的乱序由于编译器引起的指令乱序或处理器本身
处理指令的乱序.</p>

<p>当处理器是弱硬件内存模式,通常称它为weakly-ordered或weak ordering.或说
它有relaxed memory model. <strong>DEC Alpha</strong> 是
<a href="http://www.mjmwired.net/kernel/Documentation/memory-barriers.txt#2277">最具代表</a>
的弱排序的处理器.</p>

<p>C/C++的底层原子操作也呈现弱内存模型,无论代码的平台是如x86/64的强序处理
器.下面章节
<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/#memory-ordering-at-compile-time">Memory ordering at compile time</a>
会演示其弱内存模型,并说明如何强制内存顺
序来保护编译器乱序.</p>

<h3 id="section-1">数据依赖性的弱</h3>

<p>ARM和PowerPC系列的处理器内存模型和Alpha同样弱,除了它们保持
<a href="http://www.mjmwired.net/kernel/Documentation/memory-barriers.txt#305">data dependency ordering</a>.它
意味两个相依赖的<code>load</code>(load A, load B&lt;-A)被保证顺序<code>load B&lt;-A</code>总能在
<code>load A</code>之后.(A data dependency barrier is a partial ordering on interdependent loads only; it is not required to have any effect on stores, independent loads or overlapping loads.)</p>

<h3 id="section-2">强内存模型</h3>

<p>弱和强内存模型区别<a href="http://herbsutter.com/2012/08/02/strong-and-weak-hardware-memory-models/#comment-5903">存
在分歧</a>.<a href="http://preshing.com/20120930/weak-vs-strong-memory-models/">Preshing</a>
总结的定义是:</p>

<p><blockquote><p>一个强硬件内存模型是在这样的硬件上每条机器指令隐性的保证acquire and release<br/>semantics的执行.因此,当一个CPU核进行了一串写操作,每个其他的CPU核看到这<br/>些值的改变顺序与其顺序一致.</p></blockquote></p>

<p>所以也就是保证了四种内存乱序
(<a href="http://g.oswego.edu/dl/jmm/cookbook.html">LoadLoad, StoreStore, LoadStore and StoreLoad</a>)
中的3种,除了不保证StoreLoad的顺序.基于以上的定义,x86/64系列处理器基本
就是强顺序的.之后
<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/#memory-ordering-at-processor-time">Memory ordering at processor time</a>
可以看到StoreLoad在X86/64的乱序实验.</p>

<h3 id="section-3">顺序一致</h3>

<p>在顺序一致
(<a href="http://en.wikipedia.org/wiki/Sequential_consistency">Sequential consistency</a>)
的内存模型中,没有内存乱序存在.</p>

<p>如今,很难找到一个现代多核设备保证在硬件层Sequential consistency.也就早
期的386没有强大到能在运行时进行任何内存的乱序.</p>

<p>当用上层语言编程时,Sequential consistency成为一个重要的软件内存模
型.Java5和之后版本,用<code>volatile</code>声明共享变量.在C+11中,可以使用默认的顺
序约束<code>memory_order_seq_cst</code>在做原子操作时.当使用这些术语后,编译器会限
制编译乱序和插入特定CPU的指令来指定合适的memory barrier类型.</p>

<h2 id="memory-ordering-at-compile-time">Memory ordering at compile time</h2>
<p>看如下代码:</p>

<p><code>c test.c
int A, B;
void test() {
  A = B + 1;
  B = 0;
}
</code></p>

<p>不打开编译器的优化,把它编译成汇编,我们可以看到,<code>B</code>的赋值在<code>A</code>的后面,和
原程序的顺序一样.</p>

<p>``` sh
$ gcc -S -masm=intel test.c</p>

<pre><code>mov	eax, DWORD PTR B
add	eax, 1
mov	DWORD PTR A, eax
mov	DWORD PTR B, 0 ```
</code></pre>

<p>用<code>O2</code>打开优化:</p>

<p>``` sh
$ gcc -S -O2  -masm=intel test.c</p>

<pre><code>mov	eax, DWORD PTR B
mov	DWORD PTR B, 0
add	eax, 1
mov	DWORD PTR A, eax ```
</code></pre>

<p>这次编译器把<code>B</code>的赋值提到<code>A</code>的前面.为什么它可以这么做呢?内存顺序的中心
没有破坏.这样的改变并不影响单线程程序,单线程程序不能知道这样的区别.</p>

<p>但是当编写lock-free代码时,这样的编译器乱序就会引起问题.看如下例子,一个
共享的标识来表明其他共享数据是否更新:</p>

<p><code>c
int value;
int updated = 0;
void UpdateValue(int x) {
    value = x;
    update = 1;
}
</code></p>

<p>如果编译器把<code>update</code>的赋值提到<code>value</code>赋值的前面.即使在单核处理器系统中,会
有问题:在两个参数赋值的中间这个线程被中断,使得另外的程序通过<code>update</code>判
断以为<code>value</code>的值已经得到更新,实际上却没有.</p>

<h3 id="compiler-barriers">显性的Compiler Barriers</h3>
<p>一种方法是用一个特殊的被称为Compiler Barrier的指令来防止编译器优化的乱
序.以下
<a href="http://en.wikipedia.org/wiki/Memory_ordering#Compiler_memory_barrier"><code>asm volative</code></a>
是GCC中的方法.</p>

<p><code>c test_barrier.c
int A, B;
void test() {
  A = B + 1;
  asm volatile("" ::: "memory");
  B = 0;
}
</code></p>

<p>经过这样的修改,打开优化,<code>B</code>的存储将保持在要求的顺序上.</p>

<p>``` sh
$ gcc -S -O2  -masm=intel test.c</p>

<pre><code>mov	eax, DWORD PTR B
add	eax, 1
mov	DWORD PTR A, eax
mov	DWORD PTR B, 0 ```
</code></pre>

<h3 id="compiler-barriers-1">隐性的Compiler Barriers</h3>
<p>在C++11中原子库中,每个不是relaxed的原子操作同时是一个compiler barrier.</p>

<p><code>c++
int value;
std::atomic&lt;int&gt; updated(0);
void UpdateValue(int x) {
    value = x;
    // reordering is prevented here
    update.store(1, std::memory_order_release);
}
</code></p>

<p>每一个拥有compiler barrier的函数本身也是一个compiler barrier,即使它是
inline的.</p>

<p><code>c++
int a;
int b;
void DoSomething() {
    a = 1;
    UpdateValue(1);
    b = a + 1;
}
</code></p>

<p>进一步推知,大多数被调用的函数是一个compiler barrier.无论它们是否包含
memory barrier.排除inline函数,被声明为<a href="http://lwn.net/Articles/285332/"><code>pure attribution</code></a>
或当
<a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0474c/CHDHIEGF.html">link-time code generation</a>
使用时.因为编译器在编译时,并不知道<code>UpdateValue</code>的运行是否依赖于<code>a</code>或会
改变<code>a</code>的值从而影响<code>b</code>,所以编译器不会乱序它们之间的顺序.</p>

<p>可以看到,有许多隐藏的规则禁止编译指令的乱序,也防止了编译器多进一步的代
码优化,所以在某些场景
<a href="https://www.kernel.org/doc/Documentation/volatile-considered-harmful.txt">Why the “volatile” type class should not be used</a>,
来让编译器进一步优化.</p>

<h3 id="section-4">无缘由的存储</h3>

<p>有隐形的Compiler Barriers,同样GCC编译器也有无缘由的存储.来自<a href="https://gcc.gnu.org/ml/gcc/2007-10/msg00266.html">这里的实例</a>:</p>

<p>``` c
extern int v;</p>

<pre><code>void
f(int set_v)
{
  if (set_v)
    v = 1;
}
</code></pre>

<p>```</p>

<p>在i686,GCC 3.3.4–4.3.0用<code>O1</code>编译得到:</p>

<p><code>sh
            pushl   %ebp
            movl    %esp, %ebp
            cmpl    $0, 8(%ebp)
            movl    $1, %eax
            cmove   v, %eax        ; load (maybe)
            movl    %eax, v        ; store (always)
            popl    %ebp
            ret
</code></p>

<p>在单线程中,没有问题,但多线程中调用<code>f(0)</code>仅仅只是读取v的值,但中断后回去
覆盖其他线程修改的值.引起
<a href="http://www.devx.com/cplus/Article/42725">data rate</a>.在新的C++11标准中
明确禁止了这样的行为,看<a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3337.pdf">最近C+11标准进行的draft</a>§1.10.22节:</p>

<p><blockquote><p>Compiler transformations that introduce assignments to a potentially shared memory location that would not be modified by the abstract machine are generally precluded by this standard.</p></blockquote></p>

<h2 id="memory-ordering-at-processor-time">Memory ordering at processor time</h2>

<p>看一个简单的CPU乱序的简单例子,即使在强内存模型的X86/64也能看到.有两个
整数<code>X</code>和<code>Y</code>初始是0,另外两个变量r1和r2读取它们的值,两个线程并行运行,执
行如下的机器代码:</p>

<p><img class="center" src="/images/blog/2014/multithreading/ordering-example.png" width="370" height="100" title="‘ordering-example’" ></p>

<p>每个线程存储1到一个共享变量,然后把对方变量读取到一个变量或一个寄存器中.无
论哪个线程先写1到内存,另外个线程读回那个值,意味着最后r1=1或r2=1或两者
都是.但是X86/64是强内存模型,它还是允许<strong>乱序</strong>机器指令.特别,每个线程允许
延迟存储到读回之后.以致最后r1和r2能同时等于0–违反直觉的一个结果.因为
指令可能如下顺序执行:</p>

<p><img class="center" src="/images/blog/2014/multithreading/reordering-example.png" width="190" height="100" title="‘reordering-example’" ></p>

<p>写一个实例程序,实际看一下CPU的确乱序了指令.源码可以
<a href="https://github.com/shishougang/blog_multithreading/tree/master/memory_reordering">Github下载</a>.两
个读写的线程代码如下:</p>

<p>``` c++
sem_t begin_sem1;
sem_t begin_sem2;
sem_t end_sem;</p>

<p>int X, Y;
int r1, r2;</p>

<p>void *ThreadFunc1(void *param) {
  MersenneTwister random(1);
  for (;;) {
    sem_wait(&amp;begin_sem1);
    // random delay
    while (random.Integer() % 8 != 0) {
    }
    X = 1;
    asm volatile(“” ::: “memory”);  // prevent compiler ordering
    r1 = Y;
    sem_post(&amp;end_sem);
  }
  return NULL;
}</p>

<p>void *ThreadFunc2(void *param) {
  MersenneTwister random(2);
  for (;;) {
    sem_wait(&amp;begin_sem2);
    // random delay
    while (random.Integer() % 8 != 0) {
    }
    Y = 1;
    asm volatile(“” ::: “memory”);  // prevent compiler ordering
    r2 = X;
    sem_post(&amp;end_sem);
  }
  return NULL;
}
```</p>

<p>随机的延迟被插入在存储的开始处,为了交错线程的开始时间,以来达到重叠两个线程
的指令的目的.随机延迟使用线程安全的<code>MersenneTwister</code>类.汇编代码<code>asm
volatile("" ::: "memory");</code>如上节所述只是用来
<a href="http://dreamrunner.org/blog/2014/06/28/qian-tan-memory-reordering/#memory-ordering-at-compile-time">防止编译器的乱序</a>,
因为这里是要看CPU的乱序,排除编译器的乱序影响.</p>

<p>主线程如下,利用
<a href="http://pubs.opengroup.org/onlinepubs/7908799/xsh/sem_init.html">POSIX的semaphore</a>
同步它与两个子线程的同步.先让两个子线程等待,直到主线程初始化<code>X=0</code>和
<code>Y=0</code>.然后主线程等待,直到两个子线程完成操作,然后主线程检查<code>r1</code>和<code>r2</code>的
值.所以semaphore防止线程见的不同步引起的内存乱序,主线程代码如下:</p>

<p>``` c++
int main(int argc, char *argv[]) {
  sem_init(&amp;begin_sem1, 0, 0);
  sem_init(&amp;begin_sem2, 0, 0);
  sem_init(&amp;end_sem, 0, 0);</p>

<p>pthread_t thread[2];
  pthread_create(&amp;thread[0], NULL, ThreadFunc1, NULL);
  pthread_create(&amp;thread[1], NULL, ThreadFunc2, NULL);</p>

<p>int detected = 0;
  for (int i = 1; ; ++i) {
    X = 0;
    Y = 0;
    sem_post(&amp;begin_sem1);
    sem_post(&amp;begin_sem2);
    sem_wait(&amp;end_sem);
    sem_wait(&amp;end_sem);
    if (r1 == 0 &amp;&amp; r2 == 0) {
      detected++;
      printf(“%d reorders detected after %d iterations\n”, detected, i);
    }
  }
  return 0;
}
```</p>

<p>在Intel i5-2435M X64的ubuntu下运行一下程序:</p>

<p><code>sh
1 reorders detected after 2181 iterations
2 reorders detected after 4575 iterations
3 reorders detected after 7689 iterations
4 reorders detected after 22215 iterations
5 reorders detected after 60023 iterations
6 reorders detected after 60499 iterations
7 reorders detected after 61639 iterations
8 reorders detected after 62243 iterations
9 reorders detected after 67998 iterations
10 reorders detected after 68098 iterations
11 reorders detected after 71179 iterations
12 reorders detected after 71668 iterations
13 reorders detected after 72417 iterations
14 reorders detected after 73970 iterations
15 reorders detected after 78227 iterations
16 reorders detected after 81897 iterations
17 reorders detected after 82722 iterations
18 reorders detected after 85377 iterations
...
</code></p>

<p>差不多每 <strong>4000</strong> 次的迭代才发现一次CPU内存乱序.所以多线程的bug是多么难
发现.那么如何消除这些乱序.至少有如下两种方法:</p>

<ol>
  <li>让两个子线程在同一个CPU核下运行.(没有可移植性方法,如下是linux平台的).</li>
  <li>使用CPU的memory barrier防止它的乱序.</li>
</ol>

<h3 id="lock-to-one-processor">Lock to one processor</h3>
<p>让两个子线程在同一个CPU核下运行,代码如下:</p>

<p><code>c++
  cpu_set_t cpus;
  CPU_ZERO(&amp;cpus);
  CPU_SET(0, &amp;cpus);
  pthread_setaffinity_np(thread[0], sizeof(cpu_set_t), &amp;cpus);
  pthread_setaffinity_np(thread[1], sizeof(cpu_set_t), &amp;cpus);
</code></p>

<h3 id="place-a-memory-barrier">Place a memory barrier</h3>

<p>防止一个Store在Load之后的乱序,需要一个StoreLoad的barrier.这里使用
<code>mfence</code>的一个全部memory barrier,防止任何类型的内存乱序.代码如下:</p>

<p><code>c++
void *ThreadFunc1(void *param) {
  MersenneTwister random(1);
  for (;;) {
    sem_wait(&amp;begin_sem1);
    // random delay
    while (random.Integer() % 8 != 0) {
    }
    X = 1;
    asm volatile("mfence" ::: "memory");  // prevent CPU ordering
    r1 = Y;
    sem_post(&amp;end_sem);
  }
  return NULL;
  }
</code></p>

<h2 id="more">More</h2>

<ol>
  <li><a href="http://www.cl.cam.ac.uk/~pes20/weakmemory/">University of Cambridge整理的文档和论文</a></li>
  <li><a href="http://lwn.net/Articles/470681/">Paul McKenney概括他们做的一些工作和工具</a></li>
  <li><a href="http://www.amazon.com/gp/product/0123973376/ref=as_li_ss_tl?ie=UTF8&amp;tag=preshonprogr-20&amp;linkCode=as2&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0123973376">The Art of Multiprocessor Programming</a></li>
  <li><a href="http://www.amazon.com/C-Concurrency-Action-Practical-Multithreading/dp/1933988770/ref=pd_sim_b_2?ie=UTF8&amp;refRID=1QTX99XZAM6HKVG7X0G2">C++ Concurrency in Action: Practical Multithreading</a></li>
  <li><a href="https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.2011.01.02a.pdf">Is Parallel Programming Hard, And, If So, What Can You Do About It?</a></li>
</ol>

<h2 id="summarization">Summarization</h2>
<ol>
  <li>有两种内存乱序存在:编译器乱序和CPU乱序.</li>
  <li>如何防止编译器乱序.</li>
  <li>如何防止CPU乱序.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[High Resolution Time]]></title>
    <link href="http://dreamrunner.org/blog/2014/06/24/high-resolution-time/"/>
    <updated>2014-06-24T20:19:36+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/06/24/high-resolution-time</id>
    <content type="html"><![CDATA[<p>在不同的平台有繁多的Time API，如何选用精准的高精度Time函数来做
performance benchmarking呢？</p>

<h2 id="wall-clock-time-vs-cpu-time">Wall-clock time VS CPU time</h2>
<p>先理解一些时间的概念。明白不同时间API测量的是什么时间。</p>

<p><a href="http://en.wikipedia.org/wiki/Wall-clock_time">Wall-clock time</a>,顾名思
义，墙上的钟，代表一个任务从开始到完成所经历的时间。它包含3部分：CPU的
时间，I/O的时间和通信延迟的时间。但wall-clock很少是正确的时钟来使用，
因为它随着时区，和daylightsaving改变，或与NTP同步。而这些特性没有一个
是有益的，如果你用它来调度任务或做performance benchmarking。它仅仅如名
字所言，墙上的一个时钟。</p>

<p><a href="http://en.wikipedia.org/wiki/CPU_time">CPU time</a>仅仅统计一个任务从开
始到完成在CPU上所花的时间。CPU time主要包括User time（在user space所花
时间）和System time（在kernel space所花时间）。</p>

<p>以并行程序为例，CPU time就是所有CPU在这个程序所花的时间总和，
Wall-clock time在这种情况可能时间相对短，它只统计任务开始到结束所花时
间。</p>

<!-- more -->

<h2 id="apif1">不同时钟API对比<sup id="fnref:f1"><a href="#fn:f1" rel="footnote">1</a></sup></h2>
<p>对于不同的时钟API,主要分析如下特性：</p>

<ol>
  <li>API测试的是什么时间？（real, user, system，CPU or wall-clock)</li>
  <li>API的精度？(s, ms, µs, or faster?)</li>
  <li>多久时间这个时钟数字会返转？或有什么策略避免它？</li>
  <li>时钟是monotonic的，还是它会随着系统时间改变（比如NTP，time zone，
daylight saving time, by the user, etc)?</li>
</ol>

<p>Linux和OS X的主要时钟API：</p>

<ul>
  <li><a href="http://linux.die.net/man/2/time">time()</a>返回系统的wall-clock，精度
到秒。</li>
  <li><a href="http://linux.die.net/man/3/clock">clock()</a>返回user和systime总共的时
间.现在标准要求<code>CLOCKS_PER_SEC</code>是<code>1000000</code>,使精度最多达到
1µs.<code>clock_t</code>类型平台相关(The range and precision of times
representable in clock_t and time_t are implementation-defined.) 它
wrap around一旦达到最大值.(通常是32位的类型,那么~2^32 ticks后,还是比
较长的时间.)</li>
  <li><a href="http://linux.die.net/man/3/clock_gettime">clock_gettime(CLOCK_MONOTONIC,..)</a>
提供纳秒级的精确度并且是单调的.它的秒和纳秒是分开存储的,所以,任何的
wrap around将很多年才发生一次.它是个不错的时钟,但OS X平台上没有.</li>
  <li><a href="http://linux.die.net/man/2/getrusage">getrusage</a>返回独立的user和
system时间,并且不会wrap around.精确达到1 µs,</li>
  <li><a href="http://linux.die.net/man/2/gettimeofday">gettimeofday</a>返回一个
wall-clock时间并达到µs精度.但是精度不能保证,因为<a href="http://www.lehman.cuny.edu/cgi-bin/man-cgi?gettimeofday+3">依赖于硬件</a>.</li>
  <li><a href="https://developer.apple.com/library/mac/qa/qa1398/_index.html">mach_absolute_time()</a>
是OS X平台的高精度(ns)计时的一个选择.ns以64位unsigned integer存储,实
际使用wrap around不是大问题,移植性是问题.</li>
</ul>

<p>Window的高精度时钟：</p>

<p><a href="http://msdn.microsoft.com/en-us/library/ms644905(VS.85).aspx">QueryPerformanceFrequency()</a>
和
<a href="http://msdn.microsoft.com/en-us/library/ms644904(v=VS.85).aspx">QueryPerformanceCounter()</a>.
QueryPerformanceFrequency() 返回计数的频率,QueryPerformanceCounter()返
回当前计数值.和Linux中CLOCK_MONOTONIC一样,它是一个稳定并单调递增计数器,精
准达到纳秒级,并且不会wrap around.</p>

<p>更多参考:</p>

<ul>
  <li><a href="https://blog.habets.se/2010/09/gettimeofday-should-never-be-used-to-measure-time">gettimeofday() should never be used to measure time</a></li>
  <li><a href="http://tdistler.com/2010/06/27/high-performance-timing-on-linux-windows">High-performance Timing on Linux / Windows</a></li>
</ul>

<h2 id="high-resolution-time">不同平台High Resolution Time</h2>

<h3 id="linux">Linux</h3>
<p>使用
<a href="http://linux.die.net/man/3/clock_gettime">clock_gettime(CLOCK_MONOTONIC,..)</a>
作为High Resolution Time,编译需加上参数<code>-lrt</code>,实例代码如下:</p>

<p>``` c clock_gettime.c
#include <time.h>
#include <stdio.h /></time.h></p>

<p>void GetMonotonicTime(struct timespec *ts) {
  clock_gettime(CLOCK_MONOTONIC, ts);
}</p>

<p>double GetElapsedTime(struct timespec *before, struct timespec *after) {
  double delta_s = after-&gt;tv_sec - before-&gt;tv_sec;
  double delta_ns = after-&gt;tv_nsec - before-&gt;tv_nsec;
  return delta_s * 1e9 + delta_ns;
}</p>

<p>int main(int argc, char *argv[]) {
  struct timespec before, after;
  GetMonotonicTime(&amp;before);
  double sum = 0.0;
  unsigned int i;
  for (i = 1; i &lt; 100; ++i) {
    sum += 1.0 / i;
  }
  GetMonotonicTime(&amp;after);
  printf(“the elapsed time=%e ns\n”, GetElapsedTime(&amp;before, &amp;after));
  return 0;
}
```</p>

<p>除了<code>clock_gettime()</code>高精度时钟外,还有相对应的高精度的睡眠函数
<a href="http://pubs.opengroup.org/onlinepubs/000095399/functions/clock_nanosleep.html">clock_nanosleep</a>,
实例代码如下:</p>

<p>``` c clock_nanosleep.c
#include <time.h /></p>

<p>int main(int argc, char *argv[])
{
  struct timespec sleep_time;
  sleep_time.tv_sec = 0;
  sleep_time.tv_nsec = 100;
  clock_nanosleep(CLOCK_REALTIME, 0, &amp;sleep_time, NULL);
  return 0;
}
```</p>

<h3 id="os-x">OS X</h3>

<h3 id="clockgettime">使用<code>clock_get_time</code></h3>
<p>``` c clock_get_time.c
#include <time.h>
#include <stdio.h>
#include &lt;mach/clock.h&gt;
#include &lt;mach/mach.h&gt;</stdio.h></time.h></p>

<p>void GetMonotonicTime(struct timespec *ts) {
  clock_serv_t cclock;
  mach_timespec_t mts;
  host_get_clock_service(mach_host_self(), SYSTEM_CLOCK, &amp;cclock);
  clock_get_time(cclock, &amp;mts);
  mach_port_deallocate(mach_task_self(), cclock);
  ts-&gt;tv_sec = mts.tv_sec;
  ts-&gt;tv_nsec = mts.tv_nsec;
}</p>

<p>double GetElapsedTime(struct timespec *before, struct timespec *after) {
  double delta_s = after-&gt;tv_sec - before-&gt;tv_sec;
  double delta_ns = after-&gt;tv_nsec - before-&gt;tv_nsec;
  return delta_s * 1e9 + delta_ns;
}</p>

<p>int main(int argc, char *argv[]) {
  struct timespec before, after;
  GetMonotonicTime(&amp;before);
  double sum = 0.0;
  unsigned int i;
  for (i = 1; i &lt; 100; ++i) {
    sum += 1.0 / i;
  }
  GetMonotonicTime(&amp;after);
  printf(“the elapsed time=%e ns\n”, GetElapsedTime(&amp;before, &amp;after));
  return 0;
}
```</p>

<h3 id="machabsolutetime">使用<code>mach_absolute_time</code></h3>
<p><code>c mach_absolute_time.c
int main(int argc, char *argv[]) {
    uint64_t        start;
    uint64_t        end;
    uint64_t        elapsed;
    Nanoseconds     elapsedNano;
    start = mach_absolute_time();
    double sum = 0.0;
    unsigned int i;
    for (i = 1; i &lt; 100; ++i) {
        sum += 1.0 / i;
    }
    end = mach_absolute_time();
    elapsed = end - start;
    // Convert to nanoseconds
    elapsedNano = AbsoluteToNanoseconds( *(AbsoluteTime *) &amp;elapsed );
}
</code></p>

<h3 id="windows">Windows</h3>
<p>``` c++ query_performance.cc
#include <iostream>
#include <windows.h> 
using namespace std;</windows.h></iostream></p>

<p>int main()
{
    LARGE_INTEGER frequency;
    LARGE_INTEGER start, end;
    double elapsedTime;</p>

<pre><code>// get ticks per second
QueryPerformanceFrequency(&amp;frequency);

QueryPerformanceCounter(&amp;start);

//do someting
double sum = 0.0;
unsigned int i;
for (i = 1; i &lt; 100; ++i) {
    sum += 1.0 / i;
}

QueryPerformanceCounter(&amp;end);

// compute and print the elapsed time in millisec
elapsedTime = (end.QuadPart - start.QuadPart) * 1000.0 / frequency.QuadPart;
cout &lt;&lt; elapsedTime &lt;&lt; " ms.\n";
return 0; } ```
</code></pre>

<div class="footnotes">
  <ol>
    <li id="fn:f1">
      <p>http://stackoverflow.com/questions/12392278/measure-time-in-linux-getrusage-vs-clock-gettime-vs-clock-vs-gettimeofday<a href="#fnref:f1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Double-Checked Locking Works In C++11]]></title>
    <link href="http://dreamrunner.org/blog/2014/06/22/double-checked-locking-works-in-c-plus-plus-11/"/>
    <updated>2014-06-22T14:07:01+08:00</updated>
    <id>http://dreamrunner.org/blog/2014/06/22/double-checked-locking-works-in-c-plus-plus-11</id>
    <content type="html"><![CDATA[<p>在
<a href="http://dreamrunner.org/blog/2014/05/03/%E6%B5%85%E8%B0%88%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F6/">浅谈设计模式六: 单例模式(Singleton)</a>
中提到double-checked locking pattern(DCLP)来实现Singleton设计模式，但是
在C++11之前，没有安全方法在可移植的C++中去实现它．具体原因可见
<a href="http://dreamrunner.org/blog/2014/05/03/%E6%B5%85%E8%B0%88%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F6/">单例模式(Singleton)</a>
或Scott Meyers和Andrei Alexandrescu发布的原文
<a href="http://www.aristeia.com/Papers/DDJ_Jul_Aug_2004_revised.pdf">“C++ and the Perils of Double-Checked Locking”</a>
．</p>

<p>C++11引入了新的内存模型和线程库，使得能在C++中实现可移植的DCLP．本文说
明如何实现它．</p>

<!-- more -->

<h2 id="double-checked-locking">什么是Double-Checked Locking</h2>
<p>在
<a href="http://dreamrunner.org/blog/2014/05/03/%E6%B5%85%E8%B0%88%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F6/">单例模式(Singleton)</a>
很好的介绍什么是DCLP,这里稍作回顾.</p>

<p>线程安全的方式实现Signleton模式如下:</p>

<p><code>c++ singleton.cc
Singleton* Singleton::instance() {
  Lock lock;    // acquire lock (params omitted for simplicity)
  if(pInstance == NULL) {
    pInstance = new Singleton();
  }
  return pInstance;
  }  // release lock (via Lock destructor)
</code></p>

<p>每次获取 Singleton 都要获取一个锁，但是实际上，我们只有当初始化 pInstance 时才需要一个锁。也就是只发生在第一次调用 instance 时。如果在一个程序运行时， instance 被调用了n次，我们只需要锁在第一次调用时。当我们知道那n-1次锁是没必要的.</p>

<p>DCLP的关键点是发现，大多数 instance 的调用将看到 pInstance 是非空的，因此根本没必要去尝试初始化它。因此，DCLP判断 pInstance 是否为空在尝试获取锁前。只有当判断成功（ pInstance 还没有被初始化）才去获取锁，然后之后这个判断在此进行一次确保 pInstance 是仍然空的。（所以名字叫双重检查锁）。第二个检查是有必要的，因为从上可以看到，另外的线程可能碰巧初始化了 pInstance 在 pInstance 被第一次判断和获取锁之间。</p>

<p><code>c++ singleton-dclp.cc
Singleton* Singleton::instance() {
  Singleton *tmp = pInstance;
  ...  // need memory barrier
  if(tmp == 0) { // 1st test
  Lock lock;
  tmp = pInstance;
  if(tmp == 0) { // 2nd test
    tmp  = new Singleton;
  ...  // need memory barrier
    pInstance = tmp;
  }
  }
return pInstance;
}
</code></p>

<p><a href="http://dreamrunner.org/blog/2014/05/03/%E6%B5%85%E8%B0%88%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F6/">单例模式(Singleton)</a>
说明了各种不安全实现的缺陷,主要原因是1) 编译器的乱序编译 和2) CPU的乱
序执行指令.所以安全的实现依靠memory barrier,防止它们的乱序,使得在多线
程中得到同步,C++11之前没有可移植的C/C++函数,但现在,C++11有了.</p>

<h2 id="c11acqurerelease-fence">使用C++11的Acqure和Release Fence</h2>
<p>使用Acqure和Release Fence来实现它,并且保证对实例<code>pInstance</code>进行原子操
作,把它定义为<code>atomic</code>类型,并用<code>memory_order_relaxed</code>操作.(Relaxed
ordering: there are no synchronization or ordering constraints, only
atomicity is required of this operation.)如下实现代码.</p>

<p>``` c++
std::atomic&lt;Singleton *&gt; Singleton::m_pInstance;
std::mutex Singleton::m_mutex;</p>

<p>Singleton* Singleton::instance() {
  Singleton *tmp = m_pInstance.load(std::memory_order_relaxed);
  std::atomic_thread_fence(std::memory_order_acquire);
  if(tmp == nullptr) {
    std::lock_guard<std::mutex> lock(m_mutex);
    tmp = m_pInstance.load(std::memory_order_relaxed);
    if(tmp == nullptr) {
      tmp  = new Singleton;
      std::atomic_thread_fence(std::memory_order_release);
      m_pInstance.store(tmp, std::memory_order_relaxed);
    }
  }
  return m_pInstance;
}
```</std::mutex></p>

<p>在多核系统中,这整个代码也是稳健的,因为memory fences在多个线程间建立了
同步的关系.<code>Singleton::m_pInstance</code>作为guard variable,singleton变
量自身成为payload.</p>

<p>如果没有这层同步关系的话,就不能保证第一个线程的所有写操作(这里就是
singleton实力的创建)被第二个线程读取到,即使<code>m_pInstance</code>已经被第二个线
程能看到.</p>

<h2 id="c11">使用C++11的底层的内存顺序约束</h2>
<p>在C++11中也可以在单元操作时附加底层的内存顺序约束来达到同样的目的.一个
write-release能同步于一个read-release.</p>

<ol>
  <li>
    <p><code>memory_order_acquire</code>: A load operation with this memory order performs the acquire operation on the affected memory location: prior writes made to other memory locations by the thread that did the release become visible in this thread.</p>
  </li>
  <li>
    <p><code>memory_order_release</code>: A store operation with this memory order performs the release operation: prior writes to other memory locations become visible to the threads that do a consume or an acquire on the same location.</p>
  </li>
</ol>

<p>``` c++
std::atomic&lt;Singleton *&gt; Singleton::m_pInstance;
std::mutex Singleton::m_mutex;</p>

<p>Singleton* Singleton::instance() {
  Singleton *tmp = m_pInstance.load(std::memory_order_acquire);
  if(tmp == nullptr) {
    std::lock_guard<std::mutex> lock(m_mutex);
    tmp = m_pInstance.load(std::memory_order_relaxed);
    if(tmp == nullptr) {
      tmp  = new Singleton;
      m_pInstance.store(tmp, std::memory_order_release);
    }
  }
  return m_pInstance;
}
```</std::mutex></p>

<p>从深层分析来看,这种形式的免锁机制的同步比上面单独memory fences来的约束
更小.这种形式的操作只意味在这个操作周围防止内存乱序,而memory fences意
味着在一块区域内防止内存乱序.更多细节参考preshing的
<a href="http://preshing.com/20131125/acquire-and-release-fences-dont-work-the-way-youd-expect/">Acquire and Release Fences Don’t Work the Way You’d Expect</a>
的分析.
## 使用C++11的Sequentially-consistent ordering
C++11还提供了其他的方法来写lock-free的代码.当在atomic操作函数中忽略
<code>std::memory_order</code>参数项,那么默认值是<code>std::memory_order_seq_cst</code>,使得
所有原子参数成为
<a href="http://en.wikipedia.org/wiki/Sequential_consistency">sequentically consistent(SC)</a>
原子.通过SC原子性,整个算法保证sequentically consistent只要没有<a href="http://www.devx.com/cplus/Article/42725">data races</a>.</p>

<p>``` c++
std::atomic&lt;Singleton *&gt; Singleton::m_pInstance;
std::mutex Singleton::m_mutex;</p>

<p>Singleton* Singleton::instance() {
  Singleton *tmp = m_pInstance.load();
  std::atomic_thread_fence(std::memory_order_acquire);
  if(tmp == nullptr) {
    std::lock_guard<std::mutex> lock(m_mutex);
    tmp = m_pInstance.load(std::memory_order_relaxed);
    if(tmp == nullptr) {
      tmp  = new Singleton;
      std::atomic_thread_fence(std::memory_order_release);
      m_pInstance.store(tmp);
    }
  }
  return m_pInstance;
}
```</std::mutex></p>

<p>SC的原子性可能更容易理解.权衡点就是它产生的机器代码没有之前做法的高效.比
如如下是Gcc 4.8.2 intle X64对上面代码产生的机器代码,通过<code>g++ -O2 -std=c++11 -S</code>.
<img src="/images/blog/2014/multithreading/sc.png" title="sc’" ></p>

<p>因为使用了SC原子性,对<code>m_pInstance</code>的存储实现使用了<code>mfence</code>指令,起到一
个在X64上的full memory fence.这是个更严格的指令想对于DCLP在X64上的实际
需求.一个普通的<code>mov</code>足以胜任.但也无关紧要,因为<code>mfence</code>指令也仅仅执行一
次而已,就在创建singleton的实例的代码路径上.</p>

<h2 id="more">More</h2>
<p>使用<a href="http://preshing.com">Preshing</a>的小型可移植的lock-free库,在没有C++11
的支持下,使用它的<a href="http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/#using-mintomic-fences">Mintomic Fences实现DCLP</a>.</p>

<p>更多关于C++11的multithreading库的详解见之后的文章.</p>
]]></content>
  </entry>
  
</feed>
